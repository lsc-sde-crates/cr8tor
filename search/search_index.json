{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CR8TOR - 5-Safes Compliant Data Orchestration","text":"<p>CR8TOR is a comprehensive data orchestration platform that supports semi-automated, metadata-driven movement of RO-Crate-compliant data packages across Secure Data Environment (SDE) infrastructures. The platform is inspired by and builds on the 5-Safes RO-Crate Profile to execute a common workflow (e.g., data validation, staging and publishing) to support the secure ingress of target dataset(s) into research workspaces accessible to requesting entities (researchers, clinicians).</p>"},{"location":"#platform-components","title":"Platform Components","text":"<p>CR8TOR consists of two main components working together to provide a complete data access solution:</p>"},{"location":"#cr8tor-cli","title":"CR8TOR CLI","text":"<p>A command-line interface that provides capabilities to:</p> <ul> <li>Initiate data projects based on data access requests (DAR)</li> <li>Create managed repositories within SDE operator's GitHub organizations</li> <li>Validate and approve data access requests through structured workflows</li> <li>Orchestrate data transfer operations from terminal or automated workflows (GitHub Actions)</li> <li>Build compliant RO-Crate packages following 5-Safes specifications</li> </ul>"},{"location":"#cr8tor-publisher","title":"CR8TOR Publisher","text":"<p>A microservices platform consisting of three FastAPI-based services:</p> <ul> <li>Approval Service: API gateway coordinating data access operations</li> <li>Metadata Service: Validates connections and retrieves dataset metadata</li> <li>Publish Service: Handles data extraction, staging, and production publishing</li> </ul>"},{"location":"#cr8tor-project-structure","title":"CR8TOR Project Structure","text":"<p>CR8TOR manages the execution of actions on target 'data project' GitHub repositories. Each project repository comprises a collection of TOML metadata files in the <code>./resources</code> directory that include a minimal set of properties required to represent:</p> <ul> <li>Governance Information: Project name, requesting agent, state of actions performed</li> <li>Access Information: Connectivity to source data stores and target TREs</li> <li>Metadata: Descriptions of specific dataset(s) requested</li> </ul>"},{"location":"#data-access-workflow","title":"Data Access Workflow","text":"<p>The CR8TOR platform implements a structured workflow that ensures secure and compliant data access:</p> <pre><code>graph TD\n    A[Data Access Request] --&gt; B[Project Initiation]\n    B --&gt; C[Metadata Validation]\n    C --&gt; D[Sign-Off Approval]\n    D --&gt; E[Data Staging]\n    E --&gt; F[Disclosure Review]\n    F --&gt; G[Data Publication]\n    G --&gt; H[Research Workspace Access]\n\n    I[CR8TOR CLI] --&gt; B\n    I --&gt; C\n    I --&gt; D\n    I --&gt; E\n    I --&gt; F\n    I --&gt; G\n\n    J[Publisher Services] --&gt; C\n    J --&gt; E\n    J --&gt; G</code></pre>"},{"location":"#workflow-phases","title":"Workflow Phases","text":"<ol> <li>Initiation: Create structured DAR project with governance metadata</li> <li>Validation: Verify data source connections and retrieve metadata</li> <li>Approval: Human approval for validated data access requests</li> <li>Staging: Extract and stage data in secure intermediate storage</li> <li>Disclosure: Review and approve staged data for production</li> <li>Publication: Move approved data to production research environment</li> </ol>"},{"location":"#5-safes-compliance","title":"5-Safes Compliance","text":"<p>CR8TOR ensures compliance with the 5-Safes framework through:</p> <ul> <li>Safe Projects: Structured project governance and approval workflows</li> <li>Safe People: Identity verification and role-based access controls</li> <li>Safe Data: Metadata validation and secure data handling</li> <li>Safe Settings: Controlled research environments and access policies</li> <li>Safe Outputs: Audited data transfer with integrity verification</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#for-data-requesters","title":"For Data Requesters","text":"<ol> <li>Review the User Guide for creating data access requests</li> <li>Understand the approval workflow and requirements</li> <li>Learn about troubleshooting common issues</li> </ol>"},{"location":"#for-sde-operators","title":"For SDE Operators","text":"<ol> <li>Set up the orchestration layer</li> <li>Configure source systems for data access</li> <li>Deploy and configure the Publisher services</li> </ol>"},{"location":"#for-developers","title":"For Developers","text":"<ol> <li>Review the CLI development guide</li> <li>Explore the command reference</li> <li>Understand the service architecture</li> </ol>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>The platform architecture separates concerns between:</p> <ul> <li>Command Layer: CR8TOR CLI for user interactions and workflow orchestration</li> <li>Service Layer: Publisher microservices for data operations</li> <li>Storage Layer: Staging and production storage with integrity verification</li> <li>Integration Layer: GitHub, Azure services, and external data sources</li> </ul>"},{"location":"#support-and-documentation","title":"Support and Documentation","text":"<ul> <li>GitHub Repositories:</li> <li>CR8TOR CLI</li> <li>CR8TOR Publisher</li> <li>Issue Tracking: Use GitHub Issues for bug reports and feature requests</li> <li>Documentation: This site provides comprehensive documentation for all components</li> </ul> <p>CR8TOR is developed and maintained by the Lancashire and South Cumbria Secure Data Environment (LSC SDE) team.</p>"},{"location":"cr8tor-cli/branching-strategy/","title":"Branching strategy","text":""},{"location":"cr8tor-cli/branching-strategy/#overview","title":"Overview","text":"<p>We follow the branching strategy defined in latest Azure Devops best practises https://learn.microsoft.com/en-us/azure/devops/repos/git/git-branching-guidance?view=azure-devops#use-feature-branches-for-your-work. This methodology highly resembles Trunk-based development.</p> <p></p> Warning <p>main branch is the PRODUCTION/LIVE version of the overall CR8TOR Solution!</p> <p>main branch has following restrictions:</p> <ul> <li>requires Pull Request</li> <li>source branches must be follow naming pattern: develop, feature/**, release/**, hotfix/**, bugfix/**</li> </ul> <p>We allow using develop branch as the more stable dev related branch.</p>"},{"location":"cr8tor-cli/branching-strategy/#release-strategy","title":"Release strategy","text":"<p>We use <code>main</code> branch as the PRODUCTION/LIVE version. The release of newer version of cr8tor should involve a peer review session and approval from cr8tor repository admins.</p> <p>Each DAR project repository has an Orchestrator workflow which uses reusable workflow from the cr8tor repository. By default, it targets the cr8tor workflow from the <code>main</code> branch. </p> <p>If we want to test the DAR project using different branch or tag version, we need to amend the DAR project's Orchestrator workflow and the target cr8tor workflow as shown in above screenshot.</p>"},{"location":"cr8tor-cli/commands/","title":"CR8TOR CLI Commands Reference","text":"<p>CR8TOR CLI provides a comprehensive set of commands to manage the entire data access request (DAR) lifecycle, from project initiation to data publication.</p>"},{"location":"cr8tor-cli/commands/#core-workflow-commands","title":"Core Workflow Commands","text":""},{"location":"cr8tor-cli/commands/#initiate-project","title":"Initiate Project","text":"<p>Initializes a new CR8 project using a specified cookiecutter template.</p> <p>Parameters:</p> Name Type Description Default <code>template_path</code> <code>str</code> <p>The GitHub URL or relative path to the cr8-cookiecutter template.                  This is prompted from the user if not provided.</p> required <code>push_to_github</code> <code>bool</code> <p>Flag to indicate if the project should be pushed to GitHub. Defaults to False.</p> <code>False</code> <code>git_org</code> <code>str</code> <p>The target GitHub organization name. Required if <code>push_to_github</code> is True.</p> <code>None</code> <code>checkout</code> <code>str</code> <p>The branch, tag, or commit to checkout from the cookiecutter template.</p> <code>None</code> <code>project_name</code> <code>str</code> <p>The name of the project to be created. If provided, cookiecutter will skip the prompt for other values.</p> <code>None</code> <code>environment</code> <code>str</code> <p>The target environment (DEV, TEST, PROD). Defaults to \"PROD\".</p> <code>'PROD'</code> <code>cr8tor_branch</code> <code>str</code> <p>For development and debugging. Specifies the GitHub cr8tor branch to be used in the orchestration layer.</p> <code>None</code> <code>runner_os</code> <code>str</code> <p>The target runner OS for GitHub Actions workflows (Windows, Linux). Defaults to \"Windows\".</p> <code>'Windows'</code> <p>This command performs the following actions: - Generates a new project by applying the specified cookiecutter template. - Adds a timestamp to the context used by the template. - If <code>push_to_github</code> is True, creates a GitHub repository under the specified organization and pushes the generated project to GitHub using the personal access token (retrieved from <code>os.getenv(\"GH_TOKEN\")</code>).</p> Example usage <p>cr8tor initiate -t https://github.com/lsc-sde-crates/cr8-cookiecutter</p> <p>cr8tor initiate -t path-to-local-cr8-cookiecutter-dir</p> <p>cr8tor initiate -t path-to-local-cr8-cookiecutter-dir -n \"my-project\" -org \"lsc-sde-crates\" --push</p> <p>cr8tor initiate -t path-to-local-cr8-cookiecutter-dir -n \"my-project\" -org \"lsc-sde-crates\" -ros \"Linux\" --push</p> Source code in <code>src/cr8tor/cli/initiate.py</code> <pre><code>@app.command(name=\"initiate\")\ndef initiate(\n    template_path: Annotated[\n        str,\n        typer.Option(\n            default=\"-t\",\n            help=\"GitHub URL or relative path to cr8-cookiecutter template\",\n            prompt=True,\n        ),\n    ],\n    push_to_github: Annotated[\n        bool,\n        typer.Option(\n            \"--push/--no-push\",\n            help=\"Flag to indicate if the project should be pushed to GitHub\",\n        ),\n    ] = False,\n    git_org: Annotated[\n        str,\n        typer.Option(\n            \"-org\",\n            help=\"Target github organisation name\",\n            hide_input=True,\n        ),\n    ] = None,\n    checkout: Annotated[\n        str,\n        typer.Option(\n            \"-chk\",\n            help=\"Branch, tag or commit to checkout from cookiecutter template\",\n        ),\n    ] = None,\n    project_name: Annotated[\n        str,\n        typer.Option(\n            \"-n\",\n            help=\"Name of the project to be created. This is optional and can be provided as an argument.\",\n        ),\n    ] = None,\n    environment: Annotated[\n        str,\n        typer.Option(\n            \"-e\",\n            help=\"Target environment. Default PROD. Must be one of the three options: DEV, TEST, PROD.\",\n            case_sensitive=False,\n            show_choices=True,\n        ),\n    ] = \"PROD\",\n    cr8tor_branch: Annotated[\n        str,\n        typer.Option(\n            \"-cb\",\n            help=\"For developing and debugging. Provide the github cr8tor branch that should be used in orchestration layer.\",\n        ),\n    ] = None,\n    runner_os: Annotated[\n        str,\n        typer.Option(\n            \"-ros\",\n            help=\"Target runner OS for GitHub Actions workflows. Must be one of: Windows, Linux.\",\n            case_sensitive=False,\n            show_choices=True,\n        ),\n    ] = \"Windows\",\n):\n    \"\"\"\n    Initializes a new CR8 project using a specified cookiecutter template.\n\n    Args:\n        template_path (str): The GitHub URL or relative path to the cr8-cookiecutter template.\n                             This is prompted from the user if not provided.\n        push_to_github (bool): Flag to indicate if the project should be pushed to GitHub. Defaults to False.\n        git_org (str, optional): The target GitHub organization name. Required if `push_to_github` is True.\n        checkout (str, optional): The branch, tag, or commit to checkout from the cookiecutter template.\n        project_name (str, optional): The name of the project to be created. If provided, cookiecutter will skip the prompt for other values.\n        environment (str): The target environment (DEV, TEST, PROD). Defaults to \"PROD\".\n        cr8tor_branch (str, optional): For development and debugging. Specifies the GitHub cr8tor branch to be used in the orchestration layer.\n        runner_os (str): The target runner OS for GitHub Actions workflows (Windows, Linux). Defaults to \"Windows\".\n\n    This command performs the following actions:\n    - Generates a new project by applying the specified cookiecutter template.\n    - Adds a timestamp to the context used by the template.\n    - If `push_to_github` is True, creates a GitHub repository under the specified organization and pushes the generated project to GitHub using the personal access token (retrieved from `os.getenv(\"GH_TOKEN\")`).\n\n    Example usage:\n        cr8tor initiate -t https://github.com/lsc-sde-crates/cr8-cookiecutter\n\n        cr8tor initiate -t path-to-local-cr8-cookiecutter-dir\n\n        cr8tor initiate -t path-to-local-cr8-cookiecutter-dir -n \"my-project\" -org \"lsc-sde-crates\" --push\n\n        cr8tor initiate -t path-to-local-cr8-cookiecutter-dir -n \"my-project\" -org \"lsc-sde-crates\" -ros \"Linux\" --push\n    \"\"\"\n    valid_environments = [\"DEV\", \"TEST\", \"PROD\"]\n    if environment.upper() not in valid_environments:\n        raise typer.BadParameter(\n            f\"Invalid environment. Choose from {valid_environments}.\"\n        )\n\n    valid_runner_os = [\"Windows\", \"Linux\"]\n    if runner_os not in valid_runner_os:\n        raise typer.BadParameter(f\"Invalid runner OS. Choose from {valid_runner_os}.\")\n\n    extra_context = {\n        \"__timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n        \"__cr8_cc_template\": template_path,\n        \"environment\": environment.upper(),\n        \"__github_cr8tor_branch\": cr8tor_branch,\n        \"runner_os\": runner_os,\n    }\n\n    # Generate the project with cookiecutter\n    if project_name is not None:\n        extra_context.update({\"project_name\": project_name})\n        extra_context.update({\"github_organization\": git_org})\n        try:\n            project_dir = cookiecutter(\n                template_path,\n                checkout=checkout,\n                extra_context=extra_context,\n                no_input=True,\n            )\n        except OutputDirExistsException as e:\n            log.info(\"Project directory already exists. Skipping creation...\")\n            # Extract folder name from exception message\n            folder_name = re.search(r'\"(.*?)\"', str(e)).group(1)\n            project_dir = Path.cwd() / folder_name\n    else:\n        try:\n            project_dir = cookiecutter(\n                template_path, checkout=checkout, extra_context=extra_context\n            )\n        except FailedHookException as e:\n            # Extract error message from the exception\n            error_msg = str(e)\n            if \"VALIDATION_ERROR:\" in error_msg:\n                validation_error = error_msg.split(\"VALIDATION_ERROR:\")[1].strip()\n                print(f\"Validation failed: {validation_error}\")\n            else:\n                print(f\"Hook failed: {error_msg}\")\n            sys.exit(1)\n        except OutputDirExistsException as e:\n            log.info(\"Project directory already exists. Skipping creation...\")\n            # Extract folder name from exception message\n            folder_name = re.search(r'\"(.*?)\"', str(e)).group(1)\n            project_dir = Path.cwd() / folder_name\n\n    resources_dir = Path(project_dir).joinpath(\"resources\")\n    project_resource_path = resources_dir.joinpath(\"governance\", \"project.toml\")\n    project_dict = project_resources.read_resource_entity(\n        project_resource_path, \"project\"\n    )\n    project_info = schemas.ProjectProps(**project_dict)\n\n    if push_to_github and git_org:\n        repo_name = project_info.reference\n\n        gh_client = gh_rest_api_client.GHApiClient(git_org)\n\n        # Create the repository and push the project to GitHub\n        gh_rest_api_client.create_and_push_project(gh_client, project_dir, repo_name)\n\n        # Check and create contributor teams\n        gh_rest_api_client.check_and_create_teams(gh_client, repo_name)\n\n        # Create repository environments for Signing Off experience\n        gh_rest_api_client.create_github_environments(gh_client, repo_name)\n</code></pre> Warning <p>--push argument requires a fine-grained PAT token generated in GitHub. It must be stored under local environment variable GH_TOKEN. See minimum PAT token permissions defined here.</p>"},{"location":"cr8tor-cli/commands/#create-project-ro-crate","title":"Create Project RO-Crate","text":"<p>Generates the initial RO-Crate data crate within the target Cr8tor project from the specified metadata resources.</p> <p>This command performs the following actions: - Generates a UUID for the project. - Builds an RO-Crate along with an RO-Crate knowledge graph. - Packages the crate as a non-serialized BagIt Archive in the \"bagit/\" directory. - If the <code>dryrun</code> option is provided, prints the crate details without writing to the \"crate/\" directory.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>str</code> <p>The agent label triggering the validation. Defaults to None.</p> <code>None</code> <code>resources_dir</code> <code>Path</code> <p>Directory containing resources to include in the RO-Crate. Defaults to \"./resources\".</p> <code>'./resources'</code> <code>bagit_dir</code> <code>Path</code> <p>Bagit directory containing the RO-Crate data directory. Defaults to \"./bagit\".</p> <code>'./bagit'</code> <code>config_file</code> <code>Path</code> <p>Location of the configuration TOML file. Defaults to \"./config.toml\".</p> <code>'./config.toml'</code> <code>dryrun</code> <code>bool</code> <p>If True, prints the crate details without writing to the \"crate/\" directory. Defaults to False.</p> <code>False</code> Example usage <p>cr8tor create -a agent_label -i path-to-resources-dir -b path-to-bagit-dir -c path-to-config-file --dryrun</p> Source code in <code>src/cr8tor/cli/create.py</code> <pre><code>@app.command(name=\"create\")\ndef create(\n    agent: Annotated[\n        str,\n        typer.Option(default=\"-a\", help=\"The agent label triggering the validation.\"),\n    ] = None,\n    resources_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-i\", help=\"Directory containing resources to include in RO-Crate.\"\n        ),\n    ] = \"./resources\",\n    bagit_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-b\", help=\"Bagit directory containing RO-Crate data directory\"\n        ),\n    ] = \"./bagit\",\n    config_file: Annotated[\n        Path, typer.Option(default=\"-c\", help=\"Location of configuration TOML file.\")\n    ] = \"./config.toml\",\n    dryrun: Annotated[bool, typer.Option(default=\"--dryrun\")] = False,\n):\n    \"\"\"\n    Generates the initial RO-Crate data crate within the target Cr8tor project from the specified metadata resources.\n\n    This command performs the following actions:\n    - Generates a UUID for the project.\n    - Builds an RO-Crate along with an RO-Crate knowledge graph.\n    - Packages the crate as a non-serialized BagIt Archive in the \"bagit/\" directory.\n    - If the `dryrun` option is provided, prints the crate details without writing to the \"crate/\" directory.\n\n    Args:\n        agent (str): The agent label triggering the validation. Defaults to None.\n        resources_dir (Path): Directory containing resources to include in the RO-Crate. Defaults to \"./resources\".\n        bagit_dir (Path): Bagit directory containing the RO-Crate data directory. Defaults to \"./bagit\".\n        config_file (Path): Location of the configuration TOML file. Defaults to \"./config.toml\".\n        dryrun (bool): If True, prints the crate details without writing to the \"crate/\" directory. Defaults to False.\n\n    Example usage:\n        cr8tor create -a agent_label -i path-to-resources-dir -b path-to-bagit-dir -c path-to-config-file --dryrun\n    \"\"\"\n\n    if agent is None:\n        agent = os.getenv(\"AGENT_USER\")\n\n    exit_msg = \"Create complete\"\n    exit_code = schemas.Cr8torReturnCode.SUCCESS\n\n    create_start_dt = datetime.now()\n    project_uuid: Annotated[\n        str,\n        \"Project UUID is a unique auto-generated identifier on creation of the project\",\n    ] = os.getenv(\"PROJECT_UUID\", str(uuid.uuid4()))\n\n    if not resources_dir.exists():\n        cli_utils.exit_command(\n            schemas.Cr8torCommandType.CREATE,\n            schemas.Cr8torReturnCode.ACTION_EXECUTION_ERROR,\n            f\"Missing resources directory at: {resources_dir}\",\n        )\n\n    project_resource_path = resources_dir.joinpath(\"governance\", \"project.toml\")\n    governance = project_resources.read_resource(project_resource_path)\n\n    if bagit_dir.exists():\n        if \"id\" in governance[\"project\"]:\n            current_rocrate_graph = proj_graph.ROCrateGraph(bagit_dir)\n            if current_rocrate_graph.is_project_action_complete(\n                command_type=schemas.Cr8torCommandType.CREATE,\n                action_type=schemas.RoCrateActionType.CREATE,\n                project_id=governance[\"project\"][\"id\"],\n            ):\n                cli_utils.exit_command(\n                    schemas.Cr8torCommandType.CREATE,\n                    schemas.Cr8torReturnCode.ACTION_EXECUTION_ERROR,\n                    \"Create command can only be run once on a project\",\n                )\n\n    governance[\"project\"].setdefault(\"id\", project_uuid)\n    governance[\"project\"].setdefault(\n        \"project_start_time\", create_start_dt.strftime(\"%Y%m%d_%H%M%S\")\n    )\n    project_resources.update_resource_entity(\n        project_resource_path, \"project\", governance[\"project\"]\n    )\n\n    project_resources.create_resource_entity(project_resource_path, \"actions\", [])\n\n    cli_utils.close_create_action_command(\n        command_type=schemas.Cr8torCommandType.CREATE,\n        start_time=create_start_dt,\n        project_id=project_uuid,\n        agent=agent,\n        project_resource_path=project_resource_path,\n        resources_dir=resources_dir,\n        exit_msg=exit_msg,\n        exit_code=exit_code,\n        instrument=os.getenv(\"APP_NAME\"),\n        result=[{\"@id\": project_uuid}],\n        dryrun=dryrun,\n        config_file=config_file,\n    )\n</code></pre>"},{"location":"cr8tor-cli/commands/#build-ro-crate-package","title":"Build RO-Crate Package","text":"<p>Builds the RO-Crate data crate for the target Cr8tor project using the specified metadata resources and configuration.</p> <p>This command performs the following actions: - Reads the configuration from the specified TOML file. - Includes resources from the specified directory into the RO-Crate. - If the <code>dryrun</code> option is provided, prints the crate details without writing to the \"crate/\" directory.</p> <p>Parameters:</p> Name Type Description Default <code>resources_dir</code> <code>Path</code> <p>Directory containing resources to include in the RO-Crate. Defaults to \"./resources\".</p> <code>'./resources'</code> <code>config_file</code> <code>Path</code> <p>Location of the configuration TOML file. Defaults to \"./config.toml\".</p> <code>'./config.toml'</code> <code>dryrun</code> <code>bool</code> <p>If True, prints the crate details without writing to the \"crate/\" directory. Defaults to False.</p> <code>False</code> Example usage <p>cr8tor build -i path-to-resources-dir -c path-to-config-file --dryrun</p> Source code in <code>src/cr8tor/cli/build.py</code> <pre><code>@app.command(name=\"build\")\ndef build(\n    resources_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-i\", help=\"Directory containing resources to include in RO-Crate.\"\n        ),\n    ] = \"./resources\",\n    config_file: Annotated[\n        Path, typer.Option(default=\"-c\", help=\"Location of configuration TOML file.\")\n    ] = \"./config.toml\",\n    dryrun: Annotated[bool, typer.Option(default=\"--dryrun\")] = False,\n):\n    \"\"\"\n    Builds the RO-Crate data crate for the target Cr8tor project using the specified metadata resources and configuration.\n\n    This command performs the following actions:\n    - Reads the configuration from the specified TOML file.\n    - Includes resources from the specified directory into the RO-Crate.\n    - If the `dryrun` option is provided, prints the crate details without writing to the \"crate/\" directory.\n\n    Args:\n        resources_dir (Path): Directory containing resources to include in the RO-Crate. Defaults to \"./resources\".\n        config_file (Path): Location of the configuration TOML file. Defaults to \"./config.toml\".\n        dryrun (bool): If True, prints the crate details without writing to the \"crate/\" directory. Defaults to False.\n\n    Example usage:\n        cr8tor build -i path-to-resources-dir -c path-to-config-file --dryrun\n    \"\"\"\n    ###############################################################################\n    # 1 Validate project build materials (i.e. resources/ &amp; config.toml)\n    ###############################################################################\n\n    config = project_resources.read_resource(config_file)\n\n    if not resources_dir.exists():\n        raise DirectoryNotFoundError(resources_dir)\n\n    project_resource_path = resources_dir.joinpath(\"governance\", \"project.toml\")\n    governance = project_resources.read_resource(project_resource_path)\n\n    access_resource_path = resources_dir.joinpath(\"access\", \"access.toml\")\n    access = project_resources.read_resource(access_resource_path)\n\n    ###############################################################################\n    # 2 Check mandatory user-defined elements (i.e. gov, access) exists before\n    #  pydantic model validation on fields\n    ###############################################################################\n\n    governance_required_keys = {\n        \"project\": f\"To build ro-crate 'project' properties must be defined in resource: {project_resource_path}\",\n        \"requesting_agent\": f\"To build ro-crate 'requesting_agent' properties must be defined in resource: {project_resource_path}\",\n        \"repository\": f\"To build ro-crate 'repository' properties must be defined in resource: {project_resource_path}\",\n        \"actions\": f\"To build ro-crate 'actions'list property must be defined in resource: {project_resource_path}\",\n    }\n\n    access_required_keys = {\n        \"source\": f\"To build ro-crate source connection info is needed in resource: {access_resource_path}\",\n        \"credentials\": f\"To build ro-crate connection credentials info is needed in resource: {access_resource_path}\",\n    }\n\n    check_required_keys(governance, governance_required_keys)\n    check_required_keys(access, access_required_keys)\n\n    ###############################################################################\n    # 3 Create initial Ro-Crate &amp; build contextual entities\n    ###############################################################################\n\n    crate = ROCrate(gen_preview=True)\n\n    #\n    # Load project info and init RC 'Project' entity\n    #\n\n    project_props = s.ProjectProps(**governance[\"project\"])\n    log.info(\n        f\"[cyan]Creating RO-Crate for[/cyan] - [bold magenta]{project_props.name}[/bold magenta]\",\n    )\n\n    project_entity = m.ContextEntity(\n        crate=crate,\n        identifier=project_props.id,\n        properties={\n            \"@type\": \"Project\",\n            \"name\": project_props.name,\n            \"identifier\": project_props.reference,\n        },\n    )\n    crate.add(project_entity)\n\n    #\n    # Load requesting agent info and init RC 'Person' entity\n    #\n    requesting_agent_props = s.AgentProps(**governance[\"requesting_agent\"])\n    person_entity = m.Person(\n        crate,\n        identifier=f\"requesting-agent-{project_props.id}\",\n        properties={\n            \"name\": requesting_agent_props.name,\n            \"affiliation\": {\"@id\": f\"requesting-agent-org-{project_props.id}\"},\n        },\n    )\n\n    aff_entity = m.ContextEntity(\n        crate,\n        identifier=f\"requesting-agent-org-{project_props.id}\",\n        properties={\n            \"@type\": \"Organisation\",\n            \"name\": requesting_agent_props.affiliation.name,\n            \"url\": str(requesting_agent_props.affiliation.url),\n        },\n    )\n\n    crate.add(aff_entity)\n    crate.add(person_entity)\n\n    # Relation definition for ro-crate metadata file only (i.e. not stored are managed in the resources)\n    project_entity[\"memberOf\"] = [{\"@id\": person_entity.id}]\n\n    #\n    # Load project repository info and init RC 'SoftwareSourceCode' entity\n    #\n    repo_props = s.SoftwareSourceCodeProps(**governance[\"repository\"])\n\n    repo_entity = m.ContextEntity(\n        crate=crate,\n        identifier=f\"repo-{project_props.id}\",\n        properties={\n            \"@type\": \"SoftwareSourceCode\",\n            \"name\": repo_props.name,\n            \"description\": repo_props.description,\n            \"codeRepository\": f\"{repo_props.codeRepository}cr8-{project_props.id}\",\n        },\n    )\n\n    crate.add(repo_entity)\n    crate.metadata[\"isBasedOn\"] = {\"@id\": f\"repo-{project_props.id}\"}\n\n    #\n    # Load access info and init RC entities\n    #\n\n    # contract_props = s.DataAccessContract(\n    #     source=s.DatabricksSourceConnection(**access[\"source\"]),\n    #     credentials=s.SourceAccessCredential(**access[\"credentials\"]),\n    #     project_name=governance[\"project\"][\"project_name\"],\n    #     project_start_time=governance[\"project\"][\"project_start_time\"],\n    #     destination_type=governance[\"project\"][\"destination\"][\"type\"],\n    #     destination_name=governance[\"project\"][\"destination\"][\"name\"],\n    #     destination_format=governance[\"project\"][\"destination\"][\"format\"],\n    #     metadata=None\n    # )\n    # TODO: Identify and init any RC contextual entities for describing data access\n\n    ###############################################################################\n    # 4 Build data entities\n    ###############################################################################\n\n    #\n    # Governance resources\n    #\n\n    crate.add_file(\n        source=project_resource_path,\n        dest_path=\"governance/project.toml\",\n        properties={\n            \"name\": project_props.name,\n            \"description\": project_props.description,\n        },\n    )\n\n    log.info(\n        msg=\"[cyan]Validated and added file[/cyan] - [bold magenta]governance/project.toml[/bold magenta]\",\n    )\n\n    #\n    # Metadata resources\n    #\n\n    for f in resources_dir.joinpath(\"metadata\").glob(\"dataset*.toml\"):\n        dataset_dict = project_resources.read_resource(f)\n        dataset_props = s.DatasetMetadata(**dataset_dict)\n\n        crate.add_file(\n            source=f,\n            dest_path=f\"metadata/{f.name}\",\n            properties={\n                \"name\": dataset_props.name,\n                \"description\": dataset_props.description,\n            },\n        )\n\n        hasparts = []\n\n        if dataset_props.staging_path is not None:\n            staging_entity = m.ContextEntity(\n                crate=crate,\n                identifier=f\"{dataset_props.name}-staging\",\n                properties={\n                    \"@type\": \"Dataset\",\n                    \"name\": f\"{dataset_props.name} (Staging)\",\n                    \"url\": f\"{dataset_props.staging_path}\",\n                    \"encodingFormat\": \"application/x-duckdb\",  # TODO: add format from project metadata\n                },\n            )\n            crate.add(staging_entity)\n            hasparts.append({\"@id\": staging_entity.id})\n\n        if dataset_props.publish_path is not None:\n            publish_entity = m.ContextEntity(\n                crate=crate,\n                identifier=f\"{dataset_props.name}-publish\",\n                properties={\n                    \"@type\": \"Dataset\",\n                    \"name\": f\"{dataset_props.name} (Publish)\",\n                    \"url\": f\"{dataset_props.publish_path}\",\n                    \"encodingFormat\": \"application/x-duckdb\",  # TODO: add format from project metadata\n                },\n            )\n            crate.add(publish_entity)\n            hasparts.append({\"@id\": publish_entity.id})\n\n        data_ctx_entity = m.ContextEntity(\n            crate=crate,\n            identifier=f\"{dataset_props.name}\",\n            properties={\n                \"@type\": \"Dataset\",\n                \"name\": f\"{dataset_props.name}\",\n                \"description\": dataset_props.description,\n                \"hasPart\": hasparts,\n            },\n        )\n\n        crate.add(data_ctx_entity)\n\n    #\n    # Access resources\n    #\n\n    source_data = {}\n    source_data[\"source\"] = access[\"source\"].copy()\n    source_data[\"source\"][\"type\"] = source_data[\"source\"][\"type\"].lower()\n    source_data[\"source\"][\"credentials\"] = access[\"credentials\"]\n    source_data[\"extract_config\"] = (\n        access[\"extract_config\"] if \"extract_config\" in access else None\n    )\n    access_source = s.SourceConnectionModel(**source_data)\n    crate.add_file(\n        source=access_resource_path,\n        dest_path=\"access/access.toml\",\n        properties={\"name\": access_source.source.type},\n    )\n\n    log.info(\n        msg=\"[cyan]Validated and added access descriptor file[/cyan] - [bold magenta]access/access.toml[/bold magenta]\",\n    )\n\n    ###############################################################################\n    # 5 Finalise Crate\n    ###############################################################################\n    crate.name = project_props.name\n    crate.description = project_props.description\n    crate.license = s.CrateMeta.License\n    crate.publisher = m.ContextEntity(\n        crate,\n        identifier=s.CrateMeta.Publisher,\n        properties={\n            \"@type\": \"Organisation\",\n            \"name\": \"LSC SDE\",\n            \"url\": repo_props.codeRepository,\n        },\n    )\n    crate.mainEntity = project_entity\n\n    ###############################################################################\n    # 6 Process and render all action entities\n    ###############################################################################\n\n    #\n    # Check for actions\n    #\n\n    for action in governance[\"actions\"]:\n        if action[\"type\"] == \"CreateAction\":\n            action_props = s.CreateActionProps(**action)\n        elif action[\"type\"] == \"AssessAction\":\n            action_props = s.AssessActionProps(**action)\n\n        crate.add_action(\n            instrument=action_props.instrument,\n            identifier=action_props.id,\n            result=[item.model_dump() for item in action_props.result],\n            properties={\n                \"@type\": action_props.type,\n                \"name\": action_props.name,\n                \"startTime\": action_props.start_time.isoformat(),\n                \"endTime\": action_props.end_time.isoformat(),\n                \"actionStatus\": action_props.action_status,\n                \"agent\": action_props.agent,\n            },\n        )\n\n    ###############################################################################\n    # 7 Add Ro-crate meta to bagit directory structure\n    ###############################################################################\n    if not dryrun:\n        bagit_dir = Path(\"./bagit\")\n\n        if bagit_dir.exists() and bagit_dir.is_dir():\n            bag = bagit.Bag(str(bagit_dir))\n\n            # Update bag info from config.toml; This does not modify the External-Identifier.\n            # Delete and recreate the bag if the External-Identifier needs to be changed.\n            bag.info.update(**config[\"bagit-info\"])\n            log.info(\"Loaded existing bag\")\n        else:\n            bag = init_bag(\n                project_id=project_props.id, bagit_dir=bagit_dir, config=config\n            )\n\n        crate.write(bagit_dir / \"data\")\n        bag.save(manifests=True)\n\n        n_payload_files = len(list(bag.payload_files()))\n        log.info(\n            f\"[cyan]RO-Crate BagIt created at[/cyan] - [bold magenta]{bagit_dir} with {n_payload_files} files.[/bold magenta]\",\n        )\n    else:\n        log.warning(\n            \"[bold red]Dry run option set. Crate will not be written to disk.[/bold red]\\n\"\n        )\n\n    print_crate(crate=crate)\n</code></pre>"},{"location":"cr8tor-cli/commands/#validate-project-metadata","title":"Validate Project Metadata","text":"<p>Validate the contents of a Bagit directory containing an RO-Crate data directory.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>str</code> <p>The agent label triggering the validation. Defaults to None.</p> <code>None</code> <code>bagit_dir</code> <code>Path</code> <p>The Bagit directory containing the RO-Crate data directory.               Defaults to \"./bagit\".</p> <code>'./bagit'</code> <code>resources_dir</code> <code>Path</code> <p>The directory containing resources to include in the RO-Crate.                   Defaults to \"./resources\".</p> <code>'./resources'</code> <p>This function performs the following: - Validates the contents of the specified Bagit directory and its RO-Crate data directory. - Validates access and governance metadata resources. - Rebuilds the Bagit contents, including the RO-Crate metadata.</p> Example usage <p>cr8tor validate -b  -i  Source code in <code>src/cr8tor/cli/validate.py</code> <pre><code>@app.command(name=\"validate\")\ndef validate(\n    agent: Annotated[\n        str,\n        typer.Option(default=\"-a\", help=\"The agent label triggering the validation.\"),\n    ] = None,\n    bagit_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-b\", help=\"Bagit directory containing RO-Crate data directory\"\n        ),\n    ] = \"./bagit\",\n    resources_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-i\", help=\"Directory containing resources to include in RO-Crate.\"\n        ),\n    ] = \"./resources\",\n):\n    \"\"\"\n    Validate the contents of a Bagit directory containing an RO-Crate data directory.\n\n    Args:\n        agent (str): The agent label triggering the validation. Defaults to None.\n        bagit_dir (Path): The Bagit directory containing the RO-Crate data directory.\n                          Defaults to \"./bagit\".\n        resources_dir (Path): The directory containing resources to include in the RO-Crate.\n                              Defaults to \"./resources\".\n\n    This function performs the following:\n    - Validates the contents of the specified Bagit directory and its RO-Crate data directory.\n    - Validates access and governance metadata resources.\n    - Rebuilds the Bagit contents, including the RO-Crate metadata.\n\n    Example usage:\n        cr8tor validate -b &lt;path-to-bagit-dir&gt; -i &lt;path-to-resources-dir&gt;\n    \"\"\"\n\n    if agent is None:\n        agent = os.getenv(\"AGENT_USER\")\n\n    exit_msg = \"Validation complete\"\n    exit_code = schemas.Cr8torReturnCode.SUCCESS\n\n    start_time = datetime.now()\n    access_resource_path = resources_dir.joinpath(\"access\", \"access.toml\")\n    project_resource_path = resources_dir.joinpath(\"governance\", \"project.toml\")\n    project_dict = project_resources.read_resource_entity(\n        project_resource_path, \"project\"\n    )\n    project_info = schemas.ProjectProps(**project_dict)\n\n    current_rocrate_graph = proj_graph.ROCrateGraph(bagit_dir)\n    if not current_rocrate_graph.is_project_action_complete(\n        command_type=schemas.Cr8torCommandType.CREATE,\n        action_type=schemas.RoCrateActionType.CREATE,\n        project_id=project_info.id,\n    ):\n        cli_utils.close_assess_action_command(\n            command_type=schemas.Cr8torCommandType.VALIDATE,\n            start_time=start_time,\n            project_id=project_info.id,\n            agent=agent,\n            project_resource_path=project_resource_path,\n            resources_dir=resources_dir,\n            exit_msg=\"The create command must be run on the target project before validation\",\n            exit_code=schemas.Cr8torReturnCode.ACTION_WORKFLOW_ERROR,\n            instrument=os.getenv(\"METADATA_NAME\"),\n        )\n\n    for dataset_meta_file in resources_dir.joinpath(\"metadata\").glob(\"dataset_*.toml\"):\n        try:\n            access = project_resources.read_resource(access_resource_path)\n            dataset_meta = project_resources.read_resource(dataset_meta_file)\n            source_data = {}\n            source_data[\"source\"] = access[\"source\"].copy()\n            source_data[\"source\"][\"type\"] = source_data[\"source\"][\"type\"].lower()\n            source_data[\"source\"][\"credentials\"] = access[\"credentials\"]\n            source_data[\"extract_config\"] = (\n                access[\"extract_config\"] if \"extract_config\" in access else None\n            )\n            access_contract = schemas.DataContractValidateRequest(\n                project_name=project_dict[\"project_name\"],\n                project_start_time=project_dict[\"project_start_time\"],\n                destination=project_dict[\"destination\"],\n                source=source_data[\"source\"],\n                extract_config=source_data.get(\"extract_config\"),\n                dataset=schemas.DatasetMetadata(**dataset_meta),\n            )\n            metadata = asyncio.run(api.validate_access(access_contract))\n            validate_dataset_info = schemas.DatasetMetadata(**metadata)\n\n        except Exception as e:\n            cli_utils.close_assess_action_command(\n                command_type=schemas.Cr8torCommandType.VALIDATE,\n                start_time=start_time,\n                project_id=project_info.id,\n                agent=agent,\n                project_resource_path=project_resource_path,\n                resources_dir=resources_dir,\n                exit_msg=f\"{str(e)}\",\n                exit_code=schemas.Cr8torReturnCode.UNKNOWN_ERROR,\n                instrument=os.getenv(\"METADATA_NAME\"),\n            )\n\n        is_valid, err = verify_tables_metadata(\n            validate_dataset_info.tables, access_contract.dataset.tables\n        )\n        if not is_valid:\n            exit_msg = err\n            exit_code = schemas.Cr8torReturnCode.VALIDATION_ERROR\n            break\n\n        merge_metadata_into_dataset(dataset_meta_file, validate_dataset_info)\n    #\n    # This assumes validate can be run multiple times on a project\n    # Ensures previous run entities for this action are cleared in \"actions\" before\n    # actions is updated with the new action entity\n    #\n\n    cli_utils.close_assess_action_command(\n        command_type=schemas.Cr8torCommandType.VALIDATE,\n        start_time=start_time,\n        project_id=project_info.id,\n        agent=agent,\n        project_resource_path=project_resource_path,\n        resources_dir=resources_dir,\n        exit_msg=exit_msg,\n        exit_code=exit_code,\n        instrument=os.getenv(\"METADATA_NAME\"),\n        additional_type=\"Semantic Validation\",\n    )\n</code></pre>"},{"location":"cr8tor-cli/commands/#approval-workflow-commands","title":"Approval Workflow Commands","text":""},{"location":"cr8tor-cli/commands/#sign-off-project","title":"Sign-Off Project","text":"<p>Logs sign-off metadata in the RO-Crate and verifies project sign-off in the approvals management platform (e.g., GitHub).</p> <p>Parameters:</p> Name Type Description Default <code>agreement_url</code> <code>str</code> <p>URL to the project sign-off event (e.g., PR event in the project's GitHub history).</p> required <code>signing_entity</code> <code>str</code> <p>The entity that agreed to sign off the project request.</p> required <code>agent</code> <code>str</code> <p>The agent label triggering the validation. Defaults to None.</p> <code>None</code> <code>bagit_dir</code> <code>Path</code> <p>Path to the Bagit directory containing the RO-Crate data directory. Defaults to \"./bagit\".</p> <code>'./bagit'</code> <code>resources_dir</code> <code>Path</code> <p>Path to the directory containing resources to include in the RO-Crate. Defaults to \"./resources\".</p> <code>'./resources'</code> <p>This command performs the following actions: - Updates the project approvals metadata in the RO-Crate. - Verifies the project sign-off in the approvals management platform.</p> Example usage <p>cr8tor sign-off -agreement  -signing-entity  -a  -b  -i  Source code in <code>src/cr8tor/cli/sign_off.py</code> <pre><code>@app.command(name=\"sign-off\")\ndef sign_off(\n    agreement_url: Annotated[\n        str,\n        typer.Option(\n            default=\"-agreement\",\n            help=\"URL to the project sign off event (i.e. PR event in project github history)\",\n        ),\n    ],\n    signing_entity: Annotated[\n        str,\n        typer.Option(\n            default=\"-signing-entity\",\n            help=\"Entity that agreed to sign off the project request.\",\n        ),\n    ],\n    agent: Annotated[\n        str,\n        typer.Option(default=\"-a\", help=\"The agent label triggering the validation.\"),\n    ] = None,\n    bagit_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-b\", help=\"Bagit directory containing RO-Crate data directory\"\n        ),\n    ] = \"./bagit\",\n    resources_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-i\", help=\"Directory containing resources to include in RO-Crate.\"\n        ),\n    ] = \"./resources\",\n):\n    \"\"\"\n    Logs sign-off metadata in the RO-Crate and verifies project sign-off in the approvals management platform (e.g., GitHub).\n\n    Args:\n        agreement_url (str): URL to the project sign-off event (e.g., PR event in the project's GitHub history).\n        signing_entity (str): The entity that agreed to sign off the project request.\n        agent (str): The agent label triggering the validation. Defaults to None.\n        bagit_dir (Path): Path to the Bagit directory containing the RO-Crate data directory. Defaults to \"./bagit\".\n        resources_dir (Path): Path to the directory containing resources to include in the RO-Crate. Defaults to \"./resources\".\n\n    This command performs the following actions:\n    - Updates the project approvals metadata in the RO-Crate.\n    - Verifies the project sign-off in the approvals management platform.\n\n    Example usage:\n        cr8tor sign-off -agreement &lt;url_to_approved_policy&gt; -signing-entity &lt;entity_name&gt; -a &lt;agent_label&gt; -b &lt;bagit_dir&gt; -i &lt;resources_dir&gt;\n    \"\"\"\n\n    if agent is None:\n        agent = os.getenv(\"APP_NAME\")\n\n    start_time = datetime.now()\n    project_resource_path = resources_dir.joinpath(\"governance\", \"project.toml\")\n    project_dict = project_resources.read_resource_entity(\n        project_resource_path, \"project\"\n    )\n    project_info = s.ProjectProps(**project_dict)\n\n    if not bagit_dir.exists():\n        cli_utils.exit_command(\n            s.Cr8torCommandType.SIGN_OFF,\n            s.Cr8torReturnCode.ACTION_EXECUTION_ERROR,\n            f\"Missing bagit directory at: {bagit_dir}\",\n        )\n\n    current_rocrate_graph = proj_graph.ROCrateGraph(bagit_dir)\n\n    if not current_rocrate_graph.is_project_action_complete(\n        command_type=s.Cr8torCommandType.VALIDATE,\n        action_type=s.RoCrateActionType.ASSESS,\n        project_id=project_info.id,\n    ):\n        cli_utils.close_assess_action_command(\n            command_type=s.Cr8torCommandType.SIGN_OFF,\n            start_time=start_time,\n            project_id=project_info.id,\n            agent=agent,\n            project_resource_path=project_resource_path,\n            resources_dir=resources_dir,\n            exit_msg=\"The project must be validated before sign off / approval\",\n            exit_code=s.Cr8torReturnCode.ACTION_WORKFLOW_ERROR,\n            instrument=f\"{signing_entity}\",\n            additional_type=\"Sign off\",\n        )\n    #\n    # Should we verify that the approved PR URI exists here?\n    #\n\n    cli_utils.close_assess_action_command(\n        command_type=s.Cr8torCommandType.SIGN_OFF,\n        start_time=start_time,\n        project_id=project_info.id,\n        agent=agent,\n        project_resource_path=project_resource_path,\n        resources_dir=resources_dir,\n        exit_msg=\"Sign off complete\",\n        exit_code=s.Cr8torReturnCode.SUCCESS,\n        instrument=f\"{signing_entity}\",\n        additional_type=\"Sign off\",\n        result=[{\"@id\": agreement_url}],\n    )\n</code></pre>"},{"location":"cr8tor-cli/commands/#disclosure-check","title":"Disclosure Check","text":"<p>Logs disclosure metadata in the RO-Crate and verifies project disclosure in the approvals management platform (e.g., GitHub).</p> <p>Parameters:</p> Name Type Description Default <code>agreement_url</code> <code>str</code> <p>URL to the project disclosure event (e.g., PR event in the project's GitHub history).</p> required <code>signing_entity</code> <code>str</code> <p>The entity that completed the disclosure check.</p> required <code>agent</code> <code>str</code> <p>The agent label triggering the validation. Defaults to None.</p> <code>None</code> <code>bagit_dir</code> <code>Path</code> <p>Path to the Bagit directory containing the RO-Crate data directory. Defaults to \"./bagit\".</p> <code>'./bagit'</code> <code>resources_dir</code> <code>Path</code> <p>Path to the directory containing resources to include in the RO-Crate. Defaults to \"./resources\".</p> <code>'./resources'</code> <p>This command performs the following actions: - Updates the project approvals metadata in the RO-Crate. - Verifies the project disclosure in the approvals management platform.</p> Example usage <p>cr8tor disclosure -agreement  -signing-entity  -a  -b  -i  Source code in <code>src/cr8tor/cli/disclosure.py</code> <pre><code>@app.command(name=\"disclosure\")\ndef disclosure(\n    agreement_url: Annotated[\n        str,\n        typer.Option(\n            default=\"-agreement\",\n            help=\"URL to disclosure action (i.e. PR event in project github history)\",\n        ),\n    ],\n    signing_entity: Annotated[\n        str,\n        typer.Option(\n            default=\"-signing-entity\",\n            help=\"Entity that completed disclosure check\",\n        ),\n    ],\n    agent: Annotated[\n        str,\n        typer.Option(default=\"-a\", help=\"The agent label triggering the validation.\"),\n    ] = None,\n    bagit_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-b\", help=\"Bagit directory containing RO-Crate data directory\"\n        ),\n    ] = \"./bagit\",\n    resources_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-i\", help=\"Directory containing resources to include in RO-Crate.\"\n        ),\n    ] = \"./resources\",\n):\n    \"\"\"\n    Logs disclosure metadata in the RO-Crate and verifies project disclosure in the approvals management platform (e.g., GitHub).\n\n    Args:\n        agreement_url (str): URL to the project disclosure event (e.g., PR event in the project's GitHub history).\n        signing_entity (str): The entity that completed the disclosure check.\n        agent (str, optional): The agent label triggering the validation. Defaults to None.\n        bagit_dir (Path): Path to the Bagit directory containing the RO-Crate data directory. Defaults to \"./bagit\".\n        resources_dir (Path): Path to the directory containing resources to include in the RO-Crate. Defaults to \"./resources\".\n\n    This command performs the following actions:\n    - Updates the project approvals metadata in the RO-Crate.\n    - Verifies the project disclosure in the approvals management platform.\n\n    Example usage:\n        cr8tor disclosure -agreement &lt;url_to_disclosure_event&gt; -signing-entity &lt;entity_name&gt; -a &lt;agent_label&gt; -b &lt;bagit_dir&gt; -i &lt;resources_dir&gt;\n    \"\"\"\n\n    if agent is None:\n        agent = os.getenv(\"AGENT_USER\")\n\n    start_time = datetime.now()\n    project_resource_path = resources_dir.joinpath(\"governance\", \"project.toml\")\n    project_dict = project_resources.read_resource_entity(\n        project_resource_path, \"project\"\n    )\n    project_info = s.ProjectProps(**project_dict)\n\n    if not bagit_dir.exists():\n        cli_utils.exit_command(\n            s.Cr8torCommandType.DISCLOSURE_CHECK,\n            s.Cr8torReturnCode.ACTION_EXECUTION_ERROR,\n            f\"Missing bagit directory at: {bagit_dir}\",\n        )\n\n    current_rocrate_graph = proj_graph.ROCrateGraph(bagit_dir)\n    if not current_rocrate_graph.is_project_action_complete(\n        command_type=s.Cr8torCommandType.STAGE_TRANSFER,\n        action_type=s.RoCrateActionType.CREATE,\n        project_id=project_info.id,\n    ):\n        cli_utils.close_assess_action_command(\n            command_type=s.Cr8torCommandType.DISCLOSURE_CHECK,\n            start_time=start_time,\n            project_id=project_info.id,\n            agent=agent,\n            project_resource_path=project_resource_path,\n            resources_dir=resources_dir,\n            exit_msg=\"The project data must be staged before disclosure checks can be completed.\",\n            exit_code=s.Cr8torReturnCode.ACTION_WORKFLOW_ERROR,\n            instrument=f\"{signing_entity}\",\n            additional_type=\"Discloure Check\",\n        )\n\n    #\n    # Should we verify that the disclosure PR ?\n    #\n\n    cli_utils.close_assess_action_command(\n        command_type=s.Cr8torCommandType.DISCLOSURE_CHECK,\n        start_time=start_time,\n        project_id=project_info.id,\n        agent=agent,\n        project_resource_path=project_resource_path,\n        resources_dir=resources_dir,\n        exit_msg=\"Disclosure checks complete\",\n        exit_code=s.Cr8torReturnCode.SUCCESS,\n        instrument=f\"{signing_entity}\",\n        additional_type=\"Disclosure Check\",\n        result=[{\"@id\": agreement_url}],\n    )\n</code></pre>"},{"location":"cr8tor-cli/commands/#data-transfer-commands","title":"Data Transfer Commands","text":""},{"location":"cr8tor-cli/commands/#stage-data-transfer","title":"Stage Data Transfer","text":"<p>Stages the data by transferring it from the specified source to the sink TRE.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>str</code> <p>The agent label triggering the validation. Defaults to None.</p> <code>None</code> <code>bagit_dir</code> <code>Path</code> <p>Path to the Bagit directory containing the RO-Crate data directory.               Defaults to \"./bagit\".</p> <code>'./bagit'</code> <code>resources_dir</code> <code>Path</code> <p>Path to the directory containing resources to include in the RO-Crate.                   Defaults to \"./resources\".</p> <code>'./resources'</code> <p>This function prepares the data transfer for the specified CR8 project by: - Validating the current RO-Crate graph. - Ensuring that all necessary resources are included.</p> Example usage <p>cr8tor stage-transfer -a agent_label -b path-to-bagit-dir -i path-to-resources-dir</p> Source code in <code>src/cr8tor/cli/stage_transfer.py</code> <pre><code>@app.command(name=\"stage-transfer\")\ndef stage_transfer(\n    agent: Annotated[\n        str,\n        typer.Option(default=\"-a\", help=\"The agent label triggering the validation.\"),\n    ] = None,\n    bagit_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-b\", help=\"Bagit directory containing RO-Crate data directory\"\n        ),\n    ] = \"./bagit\",\n    resources_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-i\", help=\"Directory containing resources to include in RO-Crate.\"\n        ),\n    ] = \"./resources\",\n):\n    \"\"\"\n    Stages the data by transferring it from the specified source to the sink TRE.\n\n    Args:\n        agent (str): The agent label triggering the validation. Defaults to None.\n        bagit_dir (Path): Path to the Bagit directory containing the RO-Crate data directory.\n                          Defaults to \"./bagit\".\n        resources_dir (Path): Path to the directory containing resources to include in the RO-Crate.\n                              Defaults to \"./resources\".\n\n    This function prepares the data transfer for the specified CR8 project by:\n    - Validating the current RO-Crate graph.\n    - Ensuring that all necessary resources are included.\n\n    Example usage:\n        cr8tor stage-transfer -a agent_label -b path-to-bagit-dir -i path-to-resources-dir\n    \"\"\"\n\n    if agent is None:\n        agent = os.getenv(\"AGENT_USER\")\n\n    exit_msg = \"Staging transfer complete\"\n    exit_code = schemas.Cr8torReturnCode.SUCCESS\n    staging_results = []\n    start_time = datetime.now()\n\n    project_resource_path = resources_dir.joinpath(\"governance\", \"project.toml\")\n    access_resource_path = resources_dir.joinpath(\"access\", \"access.toml\")\n    project_info = project_resources.read_resource(project_resource_path)\n\n    if not bagit_dir.exists():\n        cli_utils.exit_command(\n            schemas.Cr8torCommandType.DISCLOSURE_CHECK,\n            schemas.Cr8torReturnCode.ACTION_EXECUTION_ERROR,\n            f\"Missing bagit directory at: {bagit_dir}\",\n        )\n\n    current_rocrate_graph = proj_graph.ROCrateGraph(bagit_dir)\n\n    if not current_rocrate_graph.is_project_action_complete(\n        command_type=schemas.Cr8torCommandType.SIGN_OFF,\n        action_type=schemas.RoCrateActionType.ASSESS,\n        project_id=project_info[\"project\"][\"id\"],\n    ):\n        cli_utils.close_create_action_command(\n            command_type=schemas.Cr8torCommandType.STAGE_TRANSFER,\n            start_time=start_time,\n            project_id=project_info[\"project\"][\"id\"],\n            agent=agent,\n            project_resource_path=project_resource_path,\n            resources_dir=resources_dir,\n            exit_msg=\"The data project must have sign-off before staging the data transfer\",\n            exit_code=schemas.Cr8torReturnCode.ACTION_WORKFLOW_ERROR,\n            instrument=os.getenv(\"PUBLISH_NAME\"),\n        )\n\n    for dataset_meta_file in resources_dir.joinpath(\"metadata\").glob(\"dataset*.toml\"):\n        dataset_dict = project_resources.read_resource(dataset_meta_file)\n        dataset_props = schemas.DatasetMetadata(**dataset_dict)\n\n        try:\n            access = project_resources.read_resource(access_resource_path)\n            source_data = {}\n            source_data[\"source\"] = access[\"source\"].copy()\n            source_data[\"source\"][\"type\"] = source_data[\"source\"][\"type\"].lower()\n            source_data[\"source\"][\"credentials\"] = access[\"credentials\"]\n            source_data[\"extract_config\"] = (\n                access[\"extract_config\"] if \"extract_config\" in access else None\n            )\n            access_contract = schemas.DataContractTransferRequest(\n                project_name=project_info[\"project\"][\"project_name\"],\n                project_start_time=project_info[\"project\"][\"project_start_time\"],\n                destination=project_info[\"project\"][\"destination\"],\n                source=source_data[\"source\"],\n                dataset=dataset_props,\n            )\n\n            resp_dict = asyncio.run(api.stage_transfer(access_contract))\n            resp_dict[\"destination_type\"] = project_info[\"project\"][\"destination\"][\n                \"type\"\n            ]\n            validate_resp = schemas.StageTransferPayload(**resp_dict)\n\n            # TODO: Handle multiple staging locations\n            # TODO: Add error response handler for action error property\n\n            if validate_resp.data_retrieved:\n                staging_location_dict = validate_resp.data_retrieved[0].model_dump()\n                staging_location_dict[\"@id\"] = str(uuid.uuid4())\n\n                staging_results.append(staging_location_dict)\n\n                project_resources.create_resource_entity(\n                    dataset_meta_file, \"staging_path\", staging_location_dict\n                )\n\n        except Exception as e:\n            cli_utils.close_create_action_command(\n                command_type=schemas.Cr8torCommandType.STAGE_TRANSFER,\n                start_time=start_time,\n                project_id=project_info[\"project\"][\"id\"],\n                agent=agent,\n                project_resource_path=project_resource_path,\n                resources_dir=resources_dir,\n                exit_msg=f\"{str(e)}\",\n                exit_code=schemas.Cr8torReturnCode.UNKNOWN_ERROR,\n                instrument=os.getenv(\"PUBLISH_NAME\"),\n            )\n\n    cli_utils.close_create_action_command(\n        command_type=schemas.Cr8torCommandType.STAGE_TRANSFER,\n        start_time=start_time,\n        project_id=project_info[\"project\"][\"id\"],\n        agent=agent,\n        project_resource_path=project_resource_path,\n        resources_dir=resources_dir,\n        exit_msg=exit_msg,\n        exit_code=exit_code,\n        instrument=os.getenv(\"PUBLISH_NAME\"),\n        result=staging_results,\n    )\n</code></pre>"},{"location":"cr8tor-cli/commands/#publish-data","title":"Publish Data","text":"<p>Publishes the data by transferring it from staging to production storage, making it accessible to a TRE and/or authorised TRE workspace.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>str</code> <p>The agent label triggering the validation. Defaults to None.</p> <code>None</code> <code>bagit_dir</code> <code>Path</code> <p>Path to the Bagit directory containing the RO-Crate data directory. Defaults to \"./bagit\".</p> <code>'./bagit'</code> <code>resources_dir</code> <code>Path</code> <p>Path to the directory containing resources to include in the RO-Crate. Defaults to \"./resources\".</p> <code>'./resources'</code> <p>This command performs the following actions: - Transfers the staged data to production storage. - Ensures the data is accessible to the TRE or authorised TRE workspace.</p> Example usage <p>cr8tor publish -a  -b  -i  Source code in <code>src/cr8tor/cli/publish.py</code> <pre><code>@app.command(name=\"publish\")\ndef publish(\n    agent: Annotated[\n        str,\n        typer.Option(default=\"-a\", help=\"The agent label triggering the validation.\"),\n    ] = None,\n    bagit_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-b\", help=\"Bagit directory containing RO-Crate data directory\"\n        ),\n    ] = \"./bagit\",\n    resources_dir: Annotated[\n        Path,\n        typer.Option(\n            default=\"-i\", help=\"Directory containing resources to include in RO-Crate.\"\n        ),\n    ] = \"./resources\",\n):\n    \"\"\"\n    Publishes the data by transferring it from staging to production storage, making it accessible to a TRE and/or authorised TRE workspace.\n\n    Args:\n        agent (str): The agent label triggering the validation. Defaults to None.\n        bagit_dir (Path): Path to the Bagit directory containing the RO-Crate data directory. Defaults to \"./bagit\".\n        resources_dir (Path): Path to the directory containing resources to include in the RO-Crate. Defaults to \"./resources\".\n\n    This command performs the following actions:\n    - Transfers the staged data to production storage.\n    - Ensures the data is accessible to the TRE or authorised TRE workspace.\n\n    Example usage:\n        cr8tor publish -a &lt;agent_label&gt; -b &lt;path-to-bagit-dir&gt; -i &lt;path-to-resources-dir&gt;\n    \"\"\"\n    if agent is None:\n        agent = os.getenv(\"AGENT_USER\")\n\n    exit_msg = \"Publish complete\"\n    exit_code = schemas.Cr8torReturnCode.SUCCESS\n    publish_results = []\n    start_time = datetime.now()\n\n    project_resource_path = resources_dir.joinpath(\"governance\", \"project.toml\")\n    project_info = project_resources.read_resource(project_resource_path)\n\n    if not bagit_dir.exists():\n        cli_utils.exit_command(\n            schemas.Cr8torCommandType.DISCLOSURE_CHECK,\n            schemas.Cr8torReturnCode.ACTION_EXECUTION_ERROR,\n            f\"Missing bagit directory at: {bagit_dir}\",\n        )\n\n    current_rocrate_graph = proj_graph.ROCrateGraph(bagit_dir)\n    if not current_rocrate_graph.is_project_action_complete(\n        command_type=schemas.Cr8torCommandType.DISCLOSURE_CHECK,\n        action_type=schemas.RoCrateActionType.ASSESS,\n        project_id=project_info[\"project\"][\"id\"],\n    ):\n        cli_utils.close_create_action_command(\n            command_type=schemas.Cr8torCommandType.PUBLISH,\n            start_time=start_time,\n            project_id=project_info[\"project\"][\"id\"],\n            agent=agent,\n            project_resource_path=project_resource_path,\n            resources_dir=resources_dir,\n            exit_msg=\"The data project must have disclosure completed before publishing\",\n            exit_code=schemas.Cr8torReturnCode.ACTION_WORKFLOW_ERROR,\n            instrument=os.getenv(\"PUBLISH_NAME\"),\n        )\n\n    dataset_meta_file = None\n\n    # TODO: Discuss with Piotr whether the publish function should be called per dataset or per project?\n    # Currently assumes 1 dataset file in metadata\n\n    try:\n        for f in resources_dir.joinpath(\"metadata\").glob(\"dataset*.toml\"):\n            dataset_meta_file = f\n            break\n\n        publish_req = schemas.DataContractPublishRequest(\n            project_name=project_info[\"project\"][\"project_name\"],\n            project_start_time=project_info[\"project\"][\"project_start_time\"],\n            destination=project_info[\"project\"][\"destination\"],\n        )\n\n        resp_dict = asyncio.run(api.publish(publish_req))\n        resp_dict[\"destination_type\"] = project_info[\"project\"][\"destination\"][\"type\"]\n        validate_resp = schemas.PublishPayload(**resp_dict)\n        if validate_resp.data_published:\n            publish_location_dict = validate_resp.data_published[0].model_dump()\n            publish_location_dict[\"@id\"] = str(uuid.uuid4())\n\n            publish_results.append(publish_location_dict)\n\n            project_resources.create_resource_entity(\n                dataset_meta_file, \"publish_path\", publish_location_dict\n            )\n\n    except Exception as e:\n        cli_utils.close_create_action_command(\n            command_type=schemas.Cr8torCommandType.PUBLISH,\n            start_time=start_time,\n            project_id=project_info[\"project\"][\"id\"],\n            agent=agent,\n            project_resource_path=project_resource_path,\n            resources_dir=resources_dir,\n            exit_msg=f\"{str(e)}\",\n            exit_code=schemas.Cr8torReturnCode.UNKNOWN_ERROR,\n            instrument=os.getenv(\"PUBLISH_NAME\"),\n        )\n\n    cli_utils.close_create_action_command(\n        command_type=schemas.Cr8torCommandType.PUBLISH,\n        start_time=start_time,\n        project_id=project_info[\"project\"][\"id\"],\n        agent=agent,\n        project_resource_path=project_resource_path,\n        resources_dir=resources_dir,\n        exit_msg=exit_msg,\n        exit_code=exit_code,\n        instrument=os.getenv(\"PUBLISH_NAME\"),\n        result=publish_results,\n    )\n</code></pre>"},{"location":"cr8tor-cli/commands/#command-workflow","title":"Command Workflow","text":"<p>The CR8TOR commands follow a specific sequence in the data access workflow:</p> <ol> <li>initiate - Creates a new DAR project repository from cookiecutter template</li> <li>create - Initializes the project with unique identifiers and basic metadata</li> <li>build - Builds the BagIt RO-Crate package containing project metadata</li> <li>validate - Validates data source connections and retrieves metadata</li> <li>sign-off - Records approval for the validated data request</li> <li>stage-transfer - Transfers data from source to staging storage</li> <li>disclosure - Records disclosure approval for staged data</li> <li>publish - Moves data from staging to production storage</li> </ol> Command Dependencies <p>Each command typically depends on the successful completion of previous commands in the workflow. The CLI validates these dependencies and will exit with an error if prerequisite steps are missing.</p>"},{"location":"cr8tor-cli/development/","title":"Development","text":"<ol> <li>Clone the repository</li> <li>Install uv</li> <li>Run <code>uv sync</code> in the project root. This will create a <code>.venv</code> folder and install all the project dependencies.</li> <li>Activate the virtual environment with as below:</li> <li>Windows: <code>./.venv/Scripts/activate</code></li> <li>MacOs/Linux: <code>source .venv/bin/activate</code></li> <li>Run <code>uv pip install -e .[dev]</code> to install <code>cr8tor</code> in editable mode.</li> <li>Run <code>pre-commit install</code> to enable pre-commit hooks.</li> </ol>"},{"location":"cr8tor-cli/development/#usage","title":"Usage","text":"<p>A minimal example is provided in <code>examples/simple_project</code>.</p> <p>Run <code>cr8tor --help</code> for more information</p> <ol> <li>Create a new project running:</li> <li><code>uv run cr8tor initiate -t ./../cr8-cookiecutter/ -n \"newproject\" -org \"lsc-sde-crates\"</code> to omit cookiecutter prompts and use default values</li> <li><code>uv run cr8tor initiate -t ./../cr8-cookiecutter/</code> to invoke cookiecutter prompts</li> <li>Run <code>uv run cr8tor create -a GithubAction -i ./resources</code> to create a bagit package</li> <li>Run <code>uv run cr8tor validate -a GithubAction -i ./resources</code> to create a bagit package</li> <li>Run <code>uv run cr8tor sign-off -a GithubAction -agreement \"url\" -signing-entity \"entity\"</code> to add a sign-off activity</li> <li>Run <code>uv run cr8tor stage-transfer -a GithubAction -i ./resources</code> to kick off data extraction</li> <li>Run <code>uv run cr8tor disclosure -a GithubAction -agreement \"url\" -signing-entity \"entity\"</code> to add a disclosure activity</li> <li>Run <code>uv run cr8tor publish -a GithubAction -i ./resources</code> to kick off data extraction</li> </ol>"},{"location":"cr8tor-cli/development/#debugging-in-vscode","title":"Debugging in VSCode","text":"<ol> <li>Prepare launch.json with content</li> </ol> <pre><code>{\n \"version\": \"0.2.0\",\n \"configurations\": [\n     {\n         \"name\": \"Attach\",\n         \"type\": \"debugpy\",\n         \"request\": \"attach\",\n         \"justMyCode\": false,\n         \"connect\": {\n             \"host\": \"127.0.0.1\",\n             \"port\": 5678,\n         },\n         \"pathMappings\": [\n             {\n                 \"localRoot\": \"${workspaceFolder}\",\n                 \"remoteRoot\": \"${workspaceFolder}\"\n             }\n         ]\n     }\n ]\n}\n</code></pre> <ol> <li>Run command you want to debug, e.g. <code>uv run python ./../src/cr8tor/main.py create</code></li> <li>Click F5 to invoke VSCode Debugger</li> </ol>"},{"location":"cr8tor-cli/example_usage/","title":"Example usage","text":"<ol> <li>Follow the instructions in the Development to install <code>cr8tor</code>.</li> <li>If using the <code>.venv</code> in the <code>cr8tor</code> development directory, make sure the environment has been activated.</li> <li>Open a terminal and <code>cd</code> into the <code>cr8tor-examples</code> directory (parralel to cr8tor directory. mkdir the folder if does not exists). VSCode workspace should recognize it by default.</li> <li>Review the files in the resources directory. These will be used to build the RO-Crate. See Update DAR files for details on updating the files in the resources directory.</li> <li>Run <code>cr8tor initiate -t '..\\cr8-cookiecutter\\'</code> to initiate a new project. Answer the prompts.</li> <li><code>cd</code> into the project folder.</li> <li>Run <code>cr8tor create -i ./resources -a \"AgentName\" --dryrun</code> to check everything works and review the entities that will be created in the RO-Crate. The crate will be created in a new directory called <code>bagit</code> alongside the <code>resources</code> directory. Replace AgentName with the correct value.</li> <li>Now run <code>cr8tor create -i ./resources -a \"AgentName\"</code> to build the bagit crate. Replace AgentName with the correct value.</li> <li>Open <code>bagit/data/ro-crate-preview.html</code> in a browser to preview the bagit crate and its contents.</li> <li>Run <code>cr8tor read -i ./bagit</code>. If this succeeds, then the contents of the <code>bagit</code> folder form a valid RO-Crate.</li> </ol>"},{"location":"cr8tor-cookiecutter/branching-strategy/","title":"Branching strategy","text":""},{"location":"cr8tor-cookiecutter/branching-strategy/#overview","title":"Overview","text":"<p>We follow the branching strategy defined in latest Azure Devops best practises https://learn.microsoft.com/en-us/azure/devops/repos/git/git-branching-guidance?view=azure-devops#use-feature-branches-for-your-work. This methodology highly resembles Trunk-based development.</p> <p></p> Warning <p>main branch is the PRODUCTION/LIVE version for the overall CR8TOR Solution!</p> <p>main branch has following restrictions:</p> <ul> <li>requires Pull Request</li> </ul>"},{"location":"cr8tor-cookiecutter/branching-strategy/#release-strategy","title":"Release strategy","text":"<p>For cookie-cutter we do not use any GitHub Releases or Deployment or Packages. The latest version of code commited to the 'main' branch will be used by the cr8tor as a template bases for a DAR project's repository.</p>"},{"location":"cr8tor-cookiecutter/overview/","title":"Overview","text":"<p>Cookiecutter template is used for generating 5-Safes RO-Crates using Cr8tor CLI app.</p> <p></p> <p>Example use in cr8tor cli: default, with cookiecutter prompts:</p> <pre><code>uv run  cr8tor  initiate -t \"https://github.com/lsc-sde-crates/cr8-cookiecutter\"\n</code></pre> <p>The command will download cookiecutter and start the prompt: </p> <p>Example - without cookiecutter prompts</p> <pre><code>uv run  cr8tor  initiate -t \"https://github.com/lsc-sde-crates/cr8-cookiecutter\" -n \"project4\" -org \"lsc-sde-crates\" -e \"DEV\"\n</code></pre> <p>Cr8tor CLI Initiate command expects <code>template_path</code> as a parameter. It should be either the GitHub url or a relative path to the cookiecutter template. Additionally, it can accept <code>checkout</code> parameter which will be used as a reference to CookieCutter GitHub's branch, tag or a commit during repos initiation.</p> <p>After successful run, the cookiecutter should generate a new repository: </p>"},{"location":"cr8tor-publisher/approval-service/","title":"Approval Service","text":"<p>The Approval Service acts as an API gateway for the CR8TOR Publisher platform, providing a centralized entry point for all data access requests and orchestrating communication between the Metadata and Publish Services.</p>"},{"location":"cr8tor-publisher/approval-service/#overview","title":"Overview","text":"<p>The Approval Service is a FastAPI-based microservice that serves as the main interface between the CR8TOR CLI and the underlying data processing services. It handles request validation, authentication, and routing while providing a consistent API interface.</p>"},{"location":"cr8tor-publisher/approval-service/#key-features","title":"Key Features","text":"<ul> <li>API Gateway Functionality: Central routing point for all CR8TOR operations</li> <li>Request Validation: Validates incoming requests before forwarding to appropriate services</li> <li>Service Orchestration: Coordinates between Metadata and Publish Services</li> <li>Authentication Management: Handles API key-based authentication</li> <li>Error Handling: Provides consistent error responses and logging</li> <li>Response Formatting: Standardizes response formats across services</li> </ul>"},{"location":"cr8tor-publisher/approval-service/#api-endpoints","title":"API Endpoints","text":""},{"location":"cr8tor-publisher/approval-service/#1-project-validation","title":"1. Project Validation","text":"<p>Endpoint: <code>POST /project/validate</code></p> <p>Forwards validation requests to both Metadata and Publish Services to validate connections and retrieve dataset metadata.</p> <p>Purpose:</p> <ul> <li>Validates source and destination connections</li> <li>Retrieves metadata for requested datasets</li> <li>Ensures data availability before proceeding with staging</li> </ul> <p>Request Flow:</p> <ol> <li>Receives validation request from CR8TOR CLI</li> <li>Forwards request to Metadata Service for schema validation</li> <li>Forwards request to Publish Service for connection validation</li> <li>Aggregates responses and returns metadata information</li> </ol>"},{"location":"cr8tor-publisher/approval-service/#2-data-packaging","title":"2. Data Packaging","text":"<p>Endpoint: <code>POST /project/package</code></p> <p>Forwards packaging requests to the Publish Service to extract and stage data.</p> <p>Purpose:</p> <ul> <li>Initiates data extraction from source systems</li> <li>Packages data in staging storage</li> <li>Returns staging location information</li> </ul> <p>Request Flow:</p> <ol> <li>Receives packaging request from CR8TOR CLI</li> <li>Forwards request to Publish Service for data extraction</li> <li>Returns staging details including file paths and formats</li> </ol>"},{"location":"cr8tor-publisher/approval-service/#3-data-publishing","title":"3. Data Publishing","text":"<p>Endpoint: <code>POST /project/publish</code></p> <p>Forwards publishing requests to the Publish Service to move data to production storage.</p> <p>Purpose:</p> <ul> <li>Moves data from staging to production storage</li> <li>Calculates hash values for integrity verification</li> <li>Provides final data location information</li> </ul> <p>Request Flow:</p> <ol> <li>Receives publishing request from CR8TOR CLI</li> <li>Forwards request to Publish Service for production deployment</li> <li>Returns production details including file paths and hash values</li> </ol>"},{"location":"cr8tor-publisher/approval-service/#configuration","title":"Configuration","text":""},{"location":"cr8tor-publisher/approval-service/#environment-variables","title":"Environment Variables","text":"<p>The Approval Service requires the following environment variables:</p> Variable Default Description <code>METADATA_CONTAINER_NAME</code> <code>metadata-container</code> Name of the Docker container for Metadata service <code>METADATA_CONTAINER_PORT</code> <code>8002</code> Port of Metadata container exposed to other services <code>PUBLISH_CONTAINER_NAME</code> <code>publish-container</code> Name of the Docker container for Publish service <code>PUBLISH_CONTAINER_PORT</code> <code>8003</code> Port of Publish container exposed to other services <code>SECRETS_MNT_PATH</code> <code>./secrets</code> Path to the folder where secrets are mounted"},{"location":"cr8tor-publisher/approval-service/#authentication","title":"Authentication","text":"<p>The service uses static API key authentication requiring the following secrets:</p> <p>Required Secrets:</p> <ul> <li><code>approvalserviceapikey</code> - API key for accessing the Approval Service</li> <li><code>metadataserviceapikey</code> - API key for calling the Metadata Service</li> <li><code>publishserviceapikey</code> - API key for calling the Publish Service</li> </ul> <p>When working locally, secret files should be stored under the <code>SECRETS_MNT_PATH</code> folder. In production, secrets are typically managed through Azure Key Vault or similar secret management systems.</p>"},{"location":"cr8tor-publisher/approval-service/#docker-configuration","title":"Docker Configuration","text":""},{"location":"cr8tor-publisher/approval-service/#docker-network","title":"Docker Network","text":"<p>The Approval Service communicates with other services through Docker networking. Ensure all services are on the same Docker network for proper inter-service communication.</p>"},{"location":"cr8tor-publisher/approval-service/#container-communication","title":"Container Communication","text":"<p>Services communicate using container names and internal ports:</p> <ul> <li>Metadata Service: <code>http://metadata-container:8002</code></li> <li>Publish Service: <code>http://publish-container:8003</code></li> </ul>"},{"location":"cr8tor-publisher/metadata-service/","title":"Metadata Service","text":"<p>The Metadata Service is responsible for validating dataset definitions and retrieving metadata from data sources without exposing the actual data. It ensures that the metadata specified in data access requests is accurate and available.</p>"},{"location":"cr8tor-publisher/metadata-service/#overview","title":"Overview","text":"<p>The Metadata Service is a FastAPI-based microservice that specializes in:</p> <ul> <li>Validating source and destination connections</li> <li>Retrieving schema information from data sources (e.g., Databricks Unity Catalog)</li> <li>Providing table and column metadata including descriptions, data types, and names</li> <li>Ensuring metadata accuracy without data exposure</li> </ul>"},{"location":"cr8tor-publisher/metadata-service/#key-features","title":"Key Features","text":"<ul> <li>Connection Validation: Tests connectivity to source and destination systems</li> <li>Schema Retrieval: Fetches comprehensive metadata from data catalogs</li> <li>Data Type Mapping: Provides standardized data type information</li> <li>Security-First Design: Accesses only metadata, never actual data</li> <li>Databricks Integration: Native support for Databricks Unity Catalog</li> <li>Flexible Source Support: Extensible architecture for multiple data source types</li> </ul>"},{"location":"cr8tor-publisher/metadata-service/#api-endpoints","title":"API Endpoints","text":""},{"location":"cr8tor-publisher/metadata-service/#post-metadataproject","title":"POST /metadata/project","text":"<p>The primary endpoint for metadata operations.</p> <p>Purpose:</p> <ul> <li>Validates source database connections</li> <li>Retrieves metadata for requested datasets</li> <li>Returns comprehensive schema information</li> </ul> <p>Request Example:</p> <pre><code>{\n  \"project_name\": \"Pr004\",\n  \"project_start_time\": \"20250205_010101\",\n  \"destination\": {\n    \"name\": \"LSC\",\n    \"type\": \"filestore\",\n    \"format\": \"duckdb\"\n  },\n  \"source\": {\n    \"type\": \"databrickssql\",\n    \"host_url\": \"https://my-databricks-workspace.azuredatabricks.net\",\n    \"http_path\": \"/sql/1.0/warehouses/bd1395d4652aa599\",\n    \"port\": 443,\n    \"catalog\": \"catalog_name\",\n    \"credentials\": {\n      \"provider\": \"AzureKeyVault\",\n      \"spn_clientid\": \"databricksspnclientid\",\n      \"spn_secret\": \"databricksspnsecret\"\n    }\n  },\n  \"dataset\": {\n    \"schema_name\": \"example_schema_name\",\n    \"tables\": [\n      {\n        \"name\": \"person\",\n        \"columns\": [\n          {\"name\": \"person_key\"},\n          {\"name\": \"person_id\"},\n          {\"name\": \"age\"}\n        ]\n      },\n      {\n        \"name\": \"address\",\n        \"columns\": [\n          {\"name\": \"address_key\"},\n          {\"name\": \"address\"}\n        ]\n      }\n    ]\n  }\n}\n</code></pre> <p>Response Example:</p> <pre><code>{\n  \"status\": \"success\",\n  \"payload\": {\n    \"validation_status\": \"success\",\n    \"metadata\": {\n      \"schema_name\": \"example_schema_name\",\n      \"tables\": [\n        {\n          \"name\": \"person\",\n          \"description\": \"Person demographics table\",\n          \"columns\": [\n            {\n              \"name\": \"person_key\",\n              \"data_type\": \"bigint\",\n              \"description\": \"Unique person identifier\",\n              \"nullable\": false\n            },\n            {\n              \"name\": \"person_id\",\n              \"data_type\": \"string\",\n              \"description\": \"Person ID from source system\",\n              \"nullable\": true\n            },\n            {\n              \"name\": \"age\",\n              \"data_type\": \"int\",\n              \"description\": \"Person age in years\",\n              \"nullable\": true\n            }\n          ]\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"cr8tor-publisher/metadata-service/#configuration","title":"Configuration","text":""},{"location":"cr8tor-publisher/metadata-service/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>SECRETS_MNT_PATH</code> <code>./secrets</code> Path to mounted secrets folder <code>ENVIRONMENT</code> <code>local</code> Environment variable"},{"location":"cr8tor-publisher/metadata-service/#authentication","title":"Authentication","text":"<p>The service requires the following secrets for operation:</p> <p>Required Secrets:</p> <ul> <li><code>metadataserviceapikey</code> - API key for service authentication</li> </ul>"},{"location":"cr8tor-publisher/metadata-service/#data-source-support","title":"Data Source Support","text":"<p>The Metadata Service supports multiple source types for extracting metadata:</p>"},{"location":"cr8tor-publisher/metadata-service/#databricks-unity-catalog","title":"Databricks Unity Catalog","text":"<p>Primary integration with Databricks Unity Catalog providing:</p> <ul> <li>Catalog-Level Access: Browse catalogs, schemas, and tables</li> <li>Column Metadata: Data types, descriptions, and constraints</li> </ul> <p>Connection Parameters:</p> <ul> <li><code>host_url</code>: Databricks workspace URL</li> <li><code>http_path</code>: SQL warehouse HTTP path</li> <li><code>port</code>: Connection port (typically 443)</li> <li><code>catalog</code>: Target catalog name</li> </ul> <p>Authentication Methods:</p> <ul> <li>Service Principal authentication via Azure Key Vault</li> </ul> <p>Required Secrets:</p> <ul> <li>Databricks Service Principal client ID. Provide secret name, e.g. databricksspnclientid under <code>spn_clientid</code> key in access credentials input</li> <li>Databricks Service Principal secret. Provide secret name, e.g. databricksspnsecret under <code>spn_secret</code> key in access credentials input</li> </ul>"},{"location":"cr8tor-publisher/metadata-service/#sql-databases","title":"SQL Databases","text":"<p>Support for various SQL database types including:</p> <ul> <li>PostgreSQL: Open-source relational database</li> <li>MySQL: Popular open-source database system</li> <li>Microsoft SQL Server: Enterprise database system</li> </ul> <p>Connection Parameters:</p> <ul> <li><code>host_url</code>: Database server URL</li> <li><code>database</code>: Database name to connect to</li> <li><code>port</code>: Database connection port</li> </ul> <p>Authentication Methods:</p> <ul> <li>Username/password authentication via Azure Key Vault</li> </ul> <p>Required Secrets:</p> <ul> <li>Database username. Provide secret name under <code>username_key</code> in access credentials input</li> <li>Database password. Provide secret name under <code>password_key</code> in access credentials input</li> </ul> <p>Supported Source Types:</p> <ul> <li><code>postgresql</code> - PostgreSQL databases</li> <li><code>mysql</code> - MySQL databases</li> <li><code>mssql</code> / <code>sqlserver</code> - Microsoft SQL Server databases</li> </ul>"},{"location":"cr8tor-publisher/overview/","title":"CR8TOR Publisher Overview","text":"<p>The CR8TOR Publisher is a microservices-based platform that facilitates secure data access requests, enabling users to request, approve, and retrieve datasets safely and efficiently. The publisher consists of three FastAPI-based microservices that work together to orchestrate the data transfer process.</p>"},{"location":"cr8tor-publisher/overview/#architecture-components","title":"Architecture Components","text":"<p>The CR8TOR Publisher comprises three microservices:</p>"},{"location":"cr8tor-publisher/overview/#1-approval-service","title":"1. Approval Service","text":"<p>The Approval Service acts as an API gateway, taking requests from the outside world and forwarding them to the relevant services. It serves as the main entry point for all CR8TOR operations.</p> <p>Key Features:</p> <ul> <li>API gateway functionality for routing requests</li> <li>Request validation and authentication</li> <li>Coordination between Metadata and Publish services</li> <li>Centralized error handling and response formatting</li> </ul> <p>Main Endpoints:</p> <ul> <li><code>POST project/validate</code> - Validates connections and retrieves metadata</li> <li><code>POST project/package</code> - Initiates data packaging to staging</li> <li><code>POST project/publish</code> - Publishes data to production storage</li> </ul> <p>See detailed Approval Service documentation</p>"},{"location":"cr8tor-publisher/overview/#2-metadata-service","title":"2. Metadata Service","text":"<p>The Metadata Service fetches dataset metadata, including table-column level descriptions, data types, and names, without exposing the actual data.</p> <p>Key Features:</p> <ul> <li>Validates source and destination connections</li> <li>Retrieves metadata from data sources (e.g. SQL Server, MySQL, PostgreSQL, Databricks Unity Catalog)</li> <li>Provides schema information for requested datasets</li> <li>Ensures metadata accuracy without data exposure</li> </ul> <p>Main Endpoints:</p> <ul> <li><code>POST metadata/project</code> - Retrieves and validates dataset metadata</li> </ul> <p>See detailed Metadata Service documentation</p>"},{"location":"cr8tor-publisher/overview/#3-publish-service","title":"3. Publish Service","text":"<p>The Publish Service handles the actual data transfer operations, retrieving datasets from source systems and packaging them for consumption.</p> <p>Key Features:</p> <ul> <li>Data extraction from source databases (e.g. SQL Server, MySQL, PostgreSQL, Databricks Unity Catalog)</li> <li>Data packaging and format conversion (csv or DuckDB)</li> <li>Staging and production data management</li> <li>Hash calculation and integrity verification using BagIt</li> </ul> <p>Main Endpoints:</p> <ul> <li><code>POST data-publish/validate</code> - Validates source/destination connections</li> <li><code>POST data-publish/package</code> - Packages data to staging container</li> <li><code>POST data-publish/publish</code> - Publishes data to production container</li> </ul> <p>See detailed Publish Service documentation</p>"},{"location":"cr8tor-publisher/overview/#destination-type-behaviors","title":"Destination Type Behaviors","text":"<p>The Publish Service adapts its data handling behavior based on the specified project destination type:</p>"},{"location":"cr8tor-publisher/overview/#postgresql-destination","title":"PostgreSQL Destination","text":"<p>When the project destination is configured as <code>postgresql</code>, the Publish Service:</p> <ul> <li>Data Loading: Loads the source data directly into a PostgreSQL database</li> <li>OPAL Integration: Creates and configures Obiba OPAL components for secure data access:</li> <li>Creates an OPAL project for the dataset</li> <li>Establishes OPAL resources pointing to PostgreSQL tables within the project</li> <li>Creates DataSHIELD permission groups (named <code>{project_name}_group</code>)</li> <li>Assigns DataSHIELD permissions to the created groups</li> <li>Sets resource-level permissions for the groups to access project data</li> <li>Access Control: Leverages OPAL's DataSHIELD framework for secure, privacy-preserving data analysis</li> </ul>"},{"location":"cr8tor-publisher/overview/#filestore-destination","title":"Filestore Destination","text":"<p>When the project destination is configured as <code>filestore</code>, the Publish Service:</p> <ul> <li>File-based Storage: Loads data to the mounted filestore rather than a database</li> <li>Destination-Specific Storage: Uses the destination name to determine the target storage location via environment variables (e.g., <code>TARGET_STORAGE_ACCOUNT_{DESTINATION_NAME}_SDE_MNT_PATH</code>)</li> <li>Two-stage Process:</li> <li>Staging Phase: Data is first written to a staging container/filestore</li> <li>Production Phase: Data is then moved from staging to the production container/filestore</li> <li>Format Options: Data can be packaged in multiple formats (CSV or DuckDB) for flexible consumption</li> <li>BagIt Packaging: Files are organized following BagIt standards with checksums for integrity verification</li> </ul> <p>This destination-specific behavior ensures optimal data handling and access patterns for different target environments while maintaining consistent security and governance standards.</p>"},{"location":"cr8tor-publisher/overview/#required-environment-variables","title":"Required Environment Variables","text":"<p>The Publish Service requires different environment variables depending on the destination type:</p>"},{"location":"cr8tor-publisher/overview/#postgresql-destination-environment-variables","title":"PostgreSQL Destination Environment Variables","text":"<p>When using PostgreSQL as the destination, the following environment variables are required:</p> <p>OPAL Configuration:</p> <ul> <li><code>DESTINATION_OPAL_HOST</code> - The OPAL server host URL</li> <li><code>DESTINATION_OPAL_USERNAME</code> - Username for OPAL authentication</li> <li><code>DESTINATION_OPAL_PASSWORD_SECRET_NAME</code> - Name of the secret containing the OPAL password</li> <li><code>DESTINATION_OPAL_NO_SSL_VERIFY</code> - Whether to skip SSL verification (default: \"false\")</li> </ul> <p>PostgreSQL Configuration:</p> <ul> <li><code>DESTINATION_POSTGRESQL_HOST</code> - PostgreSQL server host</li> <li><code>DESTINATION_POSTGRESQL_PORT</code> - PostgreSQL server port</li> <li><code>DESTINATION_POSTGRESQL_DATABASE</code> - Target database name</li> <li><code>DESTINATION_POSTGRESQL_OPAL_READONLY_USERNAME</code> - Read-only username for OPAL resource access</li> <li><code>DESTINATION_POSTGRESQL_OPAL_READONLY_PASSWORD_SECRET_NAME</code> - Name of the secret containing the read-only password</li> </ul>"},{"location":"cr8tor-publisher/overview/#filestore-destination-environment-variables","title":"Filestore Destination Environment Variables","text":"<p>When using filestore as the destination, the system requires destination-specific environment variables for storage mount paths:</p> <p>Storage Mount Configuration:</p> <ul> <li><code>TARGET_STORAGE_ACCOUNT_{DESTINATION_NAME}_SDE_MNT_PATH</code> - Base path to the mounted storage account for the specific destination</li> </ul> <p>Where <code>{DESTINATION_NAME}</code> is the uppercase version of the destination name specified in the project configuration. For example:</p> <ul> <li>If destination name is \"LSC\", the environment variable would be <code>TARGET_STORAGE_ACCOUNT_LSC_SDE_MNT_PATH</code></li> <li>If destination name is \"NW\", the environment variable would be <code>TARGET_STORAGE_ACCOUNT_NW_SDE_MNT_PATH</code></li> </ul> <p>The system creates the following directory structure within the mounted storage:</p> <pre><code>{base_path}/\n\u251c\u2500\u2500 staging/\n\u2502   \u2514\u2500\u2500 {project_name}/\n\u2502       \u2514\u2500\u2500 {project_start_time}/\n\u2502           \u2514\u2500\u2500 data/outputs/\n\u2514\u2500\u2500 production/\n    \u2514\u2500\u2500 {project_name}/\n        \u2514\u2500\u2500 {project_start_time}/\n            \u2514\u2500\u2500 data/outputs/\n</code></pre> <p>Additional DLT Configuration:</p> <ul> <li><code>DLTHUB_PIPELINE_WORKING_DIR</code> - Working directory for DLT Hub pipeline operations</li> <li><code>DATA_WRITER__FILE_MAX_BYTES</code> - Maximum file size in bytes (default: 100MB)</li> <li><code>DATA_WRITER__DISABLE_COMPRESSION</code> - Whether to disable compression for CSV files</li> </ul>"},{"location":"cr8tor-publisher/overview/#opal-integration-details","title":"OPAL Integration Details","text":"<p>For PostgreSQL destinations, the system performs the following OPAL operations:</p> <ol> <li>Project Creation: Creates an OPAL project named after the CR8TOR project</li> <li>Group Management: Creates a DataSHIELD group named <code>{project_name}_group</code></li> <li>User Management: Ensures a default DataSHIELD user (<code>dsuser_default</code>) exists and is assigned to the group</li> <li>Resource Creation: Creates OPAL resources for each PostgreSQL table with naming pattern <code>tre_postgresql_{schema}_{table}</code></li> <li>Permission Assignment:</li> <li>Adds the group to DataSHIELD permissions with \"use\" permission</li> <li>Sets resource-level permissions for the group with \"view\" permission on the project</li> </ol> <p>The OPAL resources are configured as SQL resources pointing to the specific PostgreSQL tables, enabling secure DataSHIELD-compliant data access for approved users.</p>"},{"location":"cr8tor-publisher/overview/#data-flow-architecture","title":"Data Flow Architecture","text":"<pre><code>graph TD\n    A[CR8TOR CLI] --&gt; B[Approval Service]\n    B --&gt; C[Metadata Service]\n    B --&gt; D[Publish Service]\n    C --&gt; E[Data Source]\n    D --&gt; E\n    D --&gt; F[Staging Storage]\n    D --&gt; G[Production Storage]\n    F --&gt; G</code></pre>"},{"location":"cr8tor-publisher/overview/#deployment-architecture","title":"Deployment Architecture","text":"<p>The microservices are containerized using Docker and designed to be deployed on Kubernetes clusters such as Azure Kubernetes Service (AKS). Each service:</p> <ul> <li>Runs in its own container with isolated dependencies</li> <li>Supports volume mounting for secrets and configuration</li> <li>Provides health checks and monitoring endpoints</li> <li>Scales independently based on workload demands</li> </ul>"},{"location":"cr8tor-publisher/overview/#security-features","title":"Security Features","text":"<ul> <li>API Key Authentication: Each service uses static API keys for inter-service communication</li> <li>Secret Management: Secrets are mounted at container level and stored in a secure credential storage (e.g. Azure Key Vault)</li> <li>Data Isolation: Services operate with minimal data exposure principles</li> <li>Audit Logging: Comprehensive logging of all data access operations</li> </ul>"},{"location":"cr8tor-publisher/overview/#integration-with-cr8tor-workflow","title":"Integration with CR8TOR Workflow","text":"<p>The publisher services integrate seamlessly with the CR8TOR CLI workflow:</p> <ol> <li>Validation Phase: Metadata Service validates data source connections</li> <li>Staging Phase: Publish Service extracts and stages data</li> <li>Publication Phase: Publish Service moves data to production storage</li> </ol> Infrastructure Requirements <p>The Lancashire and Cumbria Secure Data Environment department uses Azure Kubernetes to host and run the microservices within their SDE environment. The specific Infrastructure and Kubernetes (K8S) configuration can be found here.</p>"},{"location":"cr8tor-publisher/overview/#configuration-management","title":"Configuration Management","text":"<p>All services support:</p> <ul> <li>Environment variable configuration</li> <li>Docker network communication</li> <li>Secrets mounting for sensitive data</li> <li>Configurable storage paths and endpoints</li> </ul> <p>For detailed configuration information, see the individual service documentation linked above.</p>"},{"location":"cr8tor-publisher/publish-service/","title":"Publish Service","text":"<p>The Publish Service is responsible for the actual data transfer operations, handling data extraction from source systems, packaging for staging, and final publication to production storage. This service manages the complete data movement pipeline while maintaining security and integrity.</p>"},{"location":"cr8tor-publisher/publish-service/#overview","title":"Overview","text":"<p>The Publish Service is a FastAPI-based microservice that handles:</p> <ul> <li>Data extraction from source databases (SQL Server, MySQL, PostgreSQL, Databricks Unity Catalog)</li> <li>Data packaging and format conversion (DuckDB or csv format)</li> <li>Loading data to target database (PostgreSQL)</li> <li>Staging area management for data validation</li> <li>Production data publishing with integrity verification</li> <li>Hash calculation and BagIt packaging for data integrity</li> </ul>"},{"location":"cr8tor-publisher/publish-service/#key-features","title":"Key Features","text":"<ul> <li>Multi-Source Data Extraction: Support for various data source types</li> <li>Format Conversion: Converts data to standardized formats (DuckDB)</li> <li>Staging Management: Secure intermediate storage for data validation</li> <li>Production Publishing: Final data deployment to production storage</li> <li>Integrity Verification: Hash calculation and BagIt packaging</li> <li>Pipeline Orchestration: Coordinates complex data movement workflows</li> </ul>"},{"location":"cr8tor-publisher/publish-service/#data-source-support","title":"Data Source Support","text":"<p>The Publish Service supports multiple source types for data extraction and processing:</p>"},{"location":"cr8tor-publisher/publish-service/#databricks-unity-catalog","title":"Databricks Unity Catalog","text":"<p>Primary integration with Databricks Unity Catalog providing:</p> <ul> <li>Data Extraction: Direct access to Databricks tables and views</li> <li>Catalog-Level Support: Browse catalogs, schemas, and tables</li> <li>High-Performance Access: Optimized for large-scale data operations</li> </ul> <p>Connection Parameters:</p> <ul> <li><code>host_url</code>: Databricks workspace URL</li> <li><code>http_path</code>: SQL warehouse HTTP path</li> <li><code>port</code>: Connection port (typically 443)</li> <li><code>catalog</code>: Target catalog name</li> </ul> <p>Authentication Requirements:</p> <ul> <li>Service Principal authentication via Azure Key Vault</li> <li>Required credential keys: <code>spn_clientid</code>, <code>spn_secret</code></li> </ul>"},{"location":"cr8tor-publisher/publish-service/#sql-databases","title":"SQL Databases","text":"<p>Support for various SQL database systems including:</p> <ul> <li>PostgreSQL: Open-source relational database</li> <li>MySQL: Popular open-source database system</li> <li>Microsoft SQL Server: Enterprise database system</li> </ul> <p>Connection Parameters:</p> <ul> <li><code>host_url</code>: Database server URL</li> <li><code>database</code>: Database name to connect to</li> <li><code>port</code>: Database connection port</li> </ul> <p>Authentication Requirements:</p> <ul> <li>Username/password authentication via Azure Key Vault</li> <li>Required credential keys: <code>username_key</code>, <code>password_key</code></li> </ul> <p>Supported Source Types:</p> <ul> <li><code>databrickssql</code> - Databricks Unity Catalog</li> <li><code>postgresql</code> - PostgreSQL databases</li> <li><code>mysql</code> - MySQL databases</li> <li><code>mssql</code> / <code>sqlserver</code> - Microsoft SQL Server databases</li> </ul>"},{"location":"cr8tor-publisher/publish-service/#api-endpoints","title":"API Endpoints","text":""},{"location":"cr8tor-publisher/publish-service/#1-connection-validation","title":"1. Connection Validation","text":"<p>Endpoint: <code>POST /data-publish/validate</code></p> <p>Validates source and destination connections without transferring data.</p> <p>Purpose:</p> <ul> <li>Tests connectivity to source and destination systems</li> <li>Validates authentication and permissions</li> <li>Confirms data extraction capabilities</li> </ul> <p>Request Example:</p> <pre><code>{\n  \"project_name\": \"Pr004\",\n  \"project_start_time\": \"20250205_010101\",\n  \"destination\": {\n    \"name\": \"LSC\",\n    \"type\": \"filestore\",\n    \"format\": \"duckdb\"\n  },\n  \"source\": {\n    \"type\": \"databrickssql\",\n    \"host_url\": \"https://my-databricks-workspace.azuredatabricks.net\",\n    \"http_path\": \"/sql/1.0/warehouses/bd1395d4652aa599\",\n    \"port\": 443,\n    \"catalog\": \"catalog_name\",\n    \"credentials\": {\n      \"provider\": \"AzureKeyVault\",\n      \"spn_clientid\": \"databricksspnclientid\",\n      \"spn_secret\": \"databricksspnsecret\"\n    }\n  }\n}\n</code></pre> <p>Response Example:</p> <pre><code>{\n  \"status\": \"success\",\n  \"payload\": {\n    \"validation_status\": \"success\"\n  }\n}\n</code></pre>"},{"location":"cr8tor-publisher/publish-service/#2-data-packaging","title":"2. Data Packaging","text":"<p>Endpoint: <code>POST /data-publish/package</code></p> <p>Extracts data from source systems and packages it in staging storage.</p> <p>Purpose:</p> <ul> <li>Extracts specified datasets from source systems</li> <li>Converts data to target format (DuckDB)</li> <li>Stores data in staging area for validation</li> <li>Returns staging location information</li> </ul> <p>Request Example:</p> <pre><code>{\n  \"project_name\": \"Pr004\",\n  \"project_start_time\": \"20250205_010101\",\n  \"destination\": {\n    \"name\": \"LSC\",\n    \"type\": \"filestore\",\n    \"format\": \"duckdb\"\n  },\n  \"source\": {\n    \"type\": \"databrickssql\",\n    \"host_url\": \"https://my-databricks-workspace.azuredatabricks.net\",\n    \"http_path\": \"/sql/1.0/warehouses/bd1395d4652aa599\",\n    \"port\": 443,\n    \"catalog\": \"catalog_name\",\n    \"credentials\": {\n      \"provider\": \"AzureKeyVault\",\n      \"spn_clientid\": \"databricksspnclientid\",\n      \"spn_secret\": \"databricksspnsecret\"\n    }\n  },\n  \"dataset\": {\n    \"schema_name\": \"example_schema_name\",\n    \"tables\": [\n      {\n        \"name\": \"person\",\n        \"columns\": [\n          {\"name\": \"person_key\"},\n          {\"name\": \"person_id\"},\n          {\"name\": \"age\"}\n        ]\n      },\n      {\n        \"name\": \"address\",\n        \"columns\": [\n          {\"name\": \"address_key\"},\n          {\"name\": \"address\"}\n        ]\n      }\n    ]\n  }\n}\n</code></pre> <p>Response Example:</p> <pre><code>{\n  \"status\": \"success\",\n  \"payload\": {\n    \"destination_type\": \"filestore\",\n    \"data_retrieved\": [\n      {\n        \"file_path\": \"data/outputs/database.duckdb\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"cr8tor-publisher/publish-service/#3-data-publishing","title":"3. Data Publishing","text":"<p>Endpoint: <code>POST /data-publish/publish</code></p> <p>Moves data from staging to production storage with integrity verification.</p> <p>Purpose:</p> <ul> <li>Transfers data from staging to production storage</li> <li>Calculates file hashes for integrity verification</li> <li>Creates BagIt packages for data preservation</li> <li>Returns production location and verification information</li> </ul> <p>Request Example:</p> <pre><code>{\n  \"project_name\": \"Pr004\",\n  \"project_start_time\": \"20250205_010101\",\n  \"destination\": {\n    \"name\": \"LSC\",\n    \"type\": \"filestore\",\n    \"format\": \"duckdb\"\n  }\n}\n</code></pre> <p>Response Example:</p> <pre><code>{\n  \"status\": \"success\",\n  \"payload\": {\n    \"destination_type\": \"filestore\",\n    \"data_published\": [\n      {\n        \"file_path\": \"production/data/outputs/database.duckdb\",\n        \"hash_sha256\": \"abc123...\",\n        \"size_bytes\": 1048576\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"cr8tor-publisher/publish-service/#configuration","title":"Configuration","text":""},{"location":"cr8tor-publisher/publish-service/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>TARGET_STORAGE_ACCOUNT_LSC_SDE_MNT_PATH</code> <code>./outputs/lsc-sde</code> LSC SDE storage mount path <code>TARGET_STORAGE_ACCOUNT_NW_SDE_MNT_PATH</code> <code>./outputs/nw-sde</code> NW SDE storage mount path <code>SECRETS_MNT_PATH</code> <code>./secrets</code> Path to mounted secrets folder <code>DLTHUB_PIPELINE_WORKING_DIR</code> <code>/home/appuser/dlt/pipelines</code> DltHub working directory"},{"location":"cr8tor-publisher/publish-service/#authentication","title":"Authentication","text":"<p>Required secrets for service operation:</p> <p>API Authentication:</p> <ul> <li><code>publishserviceapikey</code> - Service API key</li> </ul> <p>Data Source Authentication:</p> <p>Databricks Unity Catalog:</p> <ul> <li><code>databricksspnclientid</code> - Databricks Service Principal client ID</li> <li><code>databricksspnsecret</code> - Databricks Service Principal secret</li> </ul> <p>SQL Databases (PostgreSQL, MySQL, MSSQL):</p> <ul> <li>Database username keys - Configured via <code>username_key</code> in access credentials</li> <li>Database password keys - Configured via <code>password_key</code> in access credentials</li> </ul> <p>Storage Authentication:</p> <ul> <li>Azure Storage Account keys (if using Azure storage)</li> <li>AWS credentials (if using AWS S3)</li> <li>Service Principal credentials for storage access</li> </ul>"},{"location":"cr8tor-publisher/publish-service/#opal-integration","title":"OPAL Integration","text":"<p>Support for OPAL (Open Policy Agent Library) for:</p> <ul> <li>Policy-based data access controls</li> <li>Dynamic permission evaluation</li> <li>Audit logging and compliance</li> <li>Fine-grained access management</li> </ul>"},{"location":"developer-guide/orchestration-layer-setup/","title":"Orchestration layer setup","text":"<p>This section explains key setup up requirements for the Orchestration layer of Cr8tor Solution.</p>"},{"location":"developer-guide/orchestration-layer-setup/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Orchestration layer setup</li> <li>Table of Contents</li> <li>GitHub Runners</li> <li>GitHub Organisation settings</li> <li>GitHub Organisation secrets</li> <li>GitHub Teams<ul> <li>Adding new members</li> </ul> </li> <li>GitHub PAT token</li> <li>DAR Project Repo settings</li> </ul>"},{"location":"developer-guide/orchestration-layer-setup/#github-runners","title":"GitHub Runners","text":"<p>GitHub runners must:</p> <ul> <li> <p>be a single runner per environment. Currently, in LSC SDE Crates organisation there are three environments defined: DEV, TEST, PROD.</p> </li> <li> <p>be Windows type (if Unix based, then cr8tor orchestrator.yml workflow requires updates from pwsh commands to batch commands).</p> </li> <li> <p>have a tag with pattern <code>&lt;env&gt;-cr8tor-vm</code>, e.g. <code>dev-cr8tor-vm</code>. Required by Orchestrator workflow.</p> </li> <li><code>dev-cr8tor-vm</code></li> <li><code>test-cr8tor-vm</code></li> <li> <p><code>prod-cr8tor-vm</code></p> </li> <li> <p>have git (https://git-scm.com/downloads/win) and github cli (https://cli.github.com/) installed.</p> </li> <li> <p>set up in GitHub at Organisation level.</p> </li> </ul> <p></p>"},{"location":"developer-guide/orchestration-layer-setup/#github-organisation-settings","title":"GitHub Organisation settings","text":"<p>Apart from the default GitHub Organisation settings, make sure to have below:</p> <ul> <li>Actions -&gt; General -&gt; Workflow permissions: Read ad write permissions. Also, Allow GitHub Actions to create and approve pull requests.</li> </ul>"},{"location":"developer-guide/orchestration-layer-setup/#github-organisation-secrets","title":"GitHub Organisation secrets","text":"<p>In order to use Cr8tor-Publisher microservices (deployed to Kubernetes), we need to provide Host and Token information for Approval Service Endpoint, for each environment seperatly.</p> <p>The secrets must be stored at Organisation level and follow the name pattern:</p> <ul> <li> <p><code>APPROVALS_API_TOKEN_&lt;DEV/TEST/PROD&gt;</code></p> </li> <li> <p><code>APPROVALS_HOST_&lt;DEV/TEST/PROD&gt;</code>, for example http://localhost</p> </li> </ul> <p></p> <p>The very same API Token secrets must be stored in the Key Vault which is mounted/accessed by the Microservice (e.g. Azure Key Vault). </p>"},{"location":"developer-guide/orchestration-layer-setup/#github-teams","title":"GitHub Teams","text":"<p>The Orchestration layer relies on the GitHub Teams. There are three types of teams defined:</p> <ul> <li><code>cr8-ALL-projects-approver</code> - currently, a single group which will approve all DAR requests. Members are notified when the GitHub Orchestration workflow reaches specific environments, such as <code>signoff</code> (indicating the need for final approval before deployment) or <code>disclosure</code> (indicating readiness for public release). By default, members have READ role on assigned DAR repositories.</li> <li><code>devops_admin</code> - team is used in the CODEOWNERS file defined in each DAR repository in the .github folder. The CODEOWNERS file specifies individuals or teams responsible for reviewing changes to specific files or directories in the repository. This ensures that updates to critical files, such as workflow definitions, are reviewed and approved by the appropriate team. By default, members have WRITE role on assigned DAR repositories.</li> <li><code>cr8-&lt;project_name&gt;-contributor</code> - team created automatically during DAR repository initiation when the <code>cr8tor initiate</code> command is executed. Members have WRITE role on assigned DAR repositories.</li> </ul> <p>The <code>cr8-ALL-projects-approver</code> and <code>devops_admin</code> teams must be created manually if they do not exist in the GitHub organisation. This is because these teams are organisation-wide and not tied to a specific project, unlike <code>cr8-&lt;project_name&gt;-contributor</code>, which is created automatically during the initiation of a new DAR project.</p> <p></p>"},{"location":"developer-guide/orchestration-layer-setup/#adding-new-members","title":"Adding new members","text":"<p>Click on a chosen team <code>cr8-ALL-projects-approver</code>, <code>devops_admin</code> or <code>cr8-&lt;project_name&gt;-contributor</code>, then Add a member. Invitation to Organisation might be required.</p>"},{"location":"developer-guide/orchestration-layer-setup/#github-pat-token","title":"GitHub PAT token","text":"<p>Cr8tor Initiate command uses a Fine-grained Personal access token (PAT) to create a DAR project repository in GitHub or to create a new GitHub Team for DAR project's contributors and assigning other organisational and repository level settings.</p> <p>The fine-grained PAT token needs to be created by GitHub Organisation administrator. It should have the following permissions:</p> <p>Repository permissions:</p> <ul> <li>Administration: Read and write</li> <li>Contents: Read and write</li> <li>Metadata: Read-only</li> <li>Pull requests: Read and write</li> <li>Workflows: Read and write</li> </ul> <p>Organization permissions:</p> <ul> <li>Members: Read and write</li> </ul> <p>Steps:</p> <ol> <li>Go to GitHub Settings to generate PAT token (https://github.com/settings/personal-access-tokens/)  </li> <li>Use following settings: name: provide the value that you will easily identify PAT's permissions for. e.g. LSC-Crates-Rocrate-Scope resource owner: restrict to the GitHub Organisation containing DAR repos, e.g. lsc-sde-crates expiration: eg 366 days or less repository access: set to ALL repositories, as we use PAT token to create new private repos permissions see the above list.   Once the PAT token is created you should see below summary:  </li> <li>Go to the CR8TOR Repository and create/update the repository secret named ORG_LVL_RW_REPOS_TEAMS.   We use the secret in the Init RO-Crate Project workflow running cr8tor initate command using the Github runner.   If you want to run cr8tor commands locally, create/obtain the token with the same permission scope.  </li> </ol> Warning <p>ORG_LVL_RW_REPOS_TEAMS is only at cr8tor repository level, not organisational level.</p>"},{"location":"developer-guide/orchestration-layer-setup/#dar-project-repo-settings","title":"DAR Project Repo settings","text":"<p>Each new DAR project is automatically created with two repository level environments:</p> <ul> <li><code>signoff</code></li> <li><code>disclosure</code></li> </ul> <p>Both of them have protection rules which requires <code>cr8-ALL-projects-approver</code> to review the workflow run. This is all automatically set up on DAR repository creation when cr8tor initiate command is run.</p> <p>Environments:   Protection rules:  </p>"},{"location":"developer-guide/source-setup/","title":"Source setup","text":"<p>Cr8tor Solution supports multiple source data types including Databricks Unity Catalog and various SQL databases. The cr8tor publisher microservices (metadata-service and publish-service) can work with different source types.</p>"},{"location":"developer-guide/source-setup/#supported-source-types","title":"Supported Source Types","text":"<p>The following source types are currently supported:</p> <ul> <li>DatabricksSQL - Databricks Unity Catalog</li> <li>PostgreSQL - PostgreSQL databases</li> <li>MySQL - MySQL databases</li> <li>MSSQL - Microsoft SQL Server databases</li> </ul> <p>Each source type has different connection parameters and credential requirements as detailed below.</p>"},{"location":"developer-guide/source-setup/#databricks-unity-catalog","title":"Databricks Unity Catalog","text":"<p>If Databricks Unity Catalog is the source dataset, this will be recognised in Metadata and Publish Services as <code>databrickssql</code> type in DAR project access file.</p> <p>Connection to Databricks Unity Catalog is made with Databricks Workspace Service principal. See here how to set up the SPN, add secrets and grant relevant access and permissions.</p>"},{"location":"developer-guide/source-setup/#databricks-connection-parameters","title":"Databricks Connection Parameters","text":"<ul> <li><code>host_url</code>: Databricks workspace URL (e.g., <code>https://&lt;workspace&gt;.azuredatabricks.net</code>)</li> <li><code>http_path</code>: SQL warehouse HTTP path (e.g., <code>/sql/1.0/warehouses/&lt;warehouse-id&gt;</code>)</li> <li><code>port</code>: Connection port (default: 443)</li> <li><code>catalog</code>: Databricks Unity Catalog name</li> </ul>"},{"location":"developer-guide/source-setup/#databricks-credential-requirements","title":"Databricks Credential Requirements","text":"<p>Databricks sources use Service Principal authentication and require the following credential keys:</p> <ul> <li><code>spn_clientid</code>: Key name in secrets provider containing the Service Principal client ID</li> <li><code>spn_secret</code>: Key name in secrets provider containing the Service Principal secret</li> </ul>"},{"location":"developer-guide/source-setup/#sql-databases-postgresql-mysql-mssql","title":"SQL Databases (PostgreSQL, MySQL, MSSQL)","text":"<p>SQL database sources are recognised in Metadata and Publish Services by their respective types: <code>postgresql</code>, <code>mysql</code>, <code>mssql</code>, or <code>sqlserver</code>.</p>"},{"location":"developer-guide/source-setup/#sql-database-connection-parameters","title":"SQL Database Connection Parameters","text":"<ul> <li><code>host_url</code>: Database server URL (e.g., <code>mysql-server.example.com</code>)</li> <li><code>database</code>: Database name to connect to</li> <li><code>port</code>: Database connection port (varies by database type)</li> </ul>"},{"location":"developer-guide/source-setup/#sql-database-credential-requirements","title":"SQL Database Credential Requirements","text":"<p>SQL database sources use username/password authentication and require the following credential keys:</p> <ul> <li><code>username_key</code>: Key name in secrets provider containing the database username</li> <li><code>password_key</code>: Key name in secrets provider containing the database password</li> </ul>"},{"location":"developer-guide/source-setup/#credential-configuration-examples","title":"Credential Configuration Examples","text":""},{"location":"developer-guide/source-setup/#databricks-unity-catalog-example","title":"Databricks Unity Catalog Example","text":"<pre><code>[source]\ntype = \"databrickssql\"\nhost_url = \"https://workspace.azuredatabricks.net\"\nhttp_path = \"/sql/1.0/warehouses/warehouse-id\"\nport = 443\ncatalog = \"my_catalog\"\n\n[credentials]\nprovider = \"AzureKeyVault\"\nspn_clientid = \"databricks-spn-client-id-secret\"\nspn_secret = \"databricks-spn-secret-secret\"\n</code></pre>"},{"location":"developer-guide/source-setup/#postgresql-example","title":"PostgreSQL Example","text":"<pre><code>[source]\ntype = \"postgresql\"\nhost_url = \"postgres-server.example.com\"\ndatabase = \"my_database\"\nport = 5432\n\n[credentials]\nprovider = \"AzureKeyVault\"\nusername_key = \"postgres-username-secret\"\npassword_key = \"postgres-password-secret\"\n</code></pre>"},{"location":"developer-guide/source-setup/#mysql-example","title":"MySQL Example","text":"<pre><code>[source]\ntype = \"mysql\"\nhost_url = \"mysql-server.example.com\"\ndatabase = \"my_database\"\nport = 3306\n\n[credentials]\nprovider = \"AzureKeyVault\"\nusername_key = \"mysql-username-secret\"\npassword_key = \"mysql-password-secret\"\n</code></pre>"},{"location":"developer-guide/source-setup/#mssql-example","title":"MSSQL Example","text":"<pre><code>[source]\ntype = \"mssql\"\nhost_url = \"mssql-server.example.com\"\ndatabase = \"my_database\"\nport = 1433\n\n[credentials]\nprovider = \"AzureKeyVault\"\nusername_key = \"mssql-username-secret\"\npassword_key = \"mssql-password-secret\"\n</code></pre>"},{"location":"lscsde/azuredatashare/","title":"Azure Data Share","text":"<p>Azure Data Share (ADS) is used to distribute data between separate Azure tenancies in a safe and controlled manner. We use ADS to share the extracted data with the North West (NW) SDE. The resource, as well as the data share and invitation, is created using Bicep, alongside other required resources explained here</p> <p></p> <p>Using the environment config files, we can control whether the invitation should be sent during the release action, change the associated email, and set the invitation expiration date. If we want to send the invitation only when releasing to the Test environment, create a release branch following the lsc-sde branching strategy and update the Test.env file in that branch. Then run the workflow. </p> <p>Alternatively, create the invitation manually in Azure Data Share if you have the correct permissions on the given resource group and subscription. Follow the naming patterns defined in the Bicep templates. </p>"},{"location":"lscsde/deployment-to-kubernetes/","title":"Deployment to Kubernetes","text":"<p>LSC SDE uses Azure Kubernetes, Helm charts, and Flux for deployment and running Docker containers. The installation guide is currently available here.</p> <p>Cr8tor Publisher microservices are deployed to separate Kubernetes clusters, each dedicated to a specific environment:</p> Environment Endpoint GitHub Self-Hosted Runner DEV dev-cr8tor.xlscsde.nhs.uk dev-vm-01 TST stg-cr8tor.xlscsde.nhs.uk test-vm-01 PROD cr8tor.xlscsde.nhs.uk prod-vm-01 <p>We can release different versions of the app to each environment. Steps:</p> <ol> <li>Build the package by running the Docker build and push workflow. This runs automatically after merging to the main branch. The build creates a versioned image for each microservice.</li> </ol> <p> </p> <ol> <li> <p>Note down the version, e.g. 0.1.55, you want to deploy to the Kubernetes cluster.    Navigate to <code>iac-flux-lscsde</code> repo    https://github.com/lsc-sde/iac-flux-lscsde/blob/dev/core/helm-config.yaml https://github.com/lsc-sde/iac-flux-lscsde/blob/dev/core/image-config.yaml    Update the version in the appropriate  branch: <code>dev</code>, <code>stg</code> or <code>prod</code>.</p> </li> <li> <p>The Kubernetes process monitors changes in the respective branch and automatically updates itself when new changes are pushed.</p> </li> </ol>"},{"location":"lscsde/infrastructure/","title":"Infrastructure","text":""},{"location":"lscsde/infrastructure/#overview","title":"Overview","text":"<p>Lancashire and Cumbria SDE uses Azure for hosting resources required to run Cr8tor Publisher microservices:</p> <ul> <li> <p>Azure Keyvault</p> <ol> <li>to store API-keys for Approval, Metadata and Publish service endpoints; 2. to store connection details to source and target systems e.g. Databricks Service Principal secrets;</li> <li>secrets are mounted to kubernetes containers;</li> </ol> </li> <li> <p>Storage Accounts</p> <ol> <li>to store the extracted data.</li> <li>two containers:<ol> <li>staging - to store the raw extract,</li> <li>production - to store the final data files (e.g. without any DLTHub log files).</li> </ol> </li> <li>separate storage accounts per target organisation;<ol> <li>one for LSC,</li> <li>one for NW.</li> </ol> </li> </ol> </li> <li> <p>Azure Data Share</p> <ol> <li>to safely serve data to trused organisations,</li> <li>created to share 'production' container of NW Storage account with the North West Azure Data Share</li> <li>invitation can be shared using IAC (there is a Bicep code for that).</li> </ol> </li> <li> <p>Private endpoints.</p> <ol> <li>used to connect Kubernetes to Storage Accounts in a safe way</li> </ol> </li> </ul> <p>The Configuration and Release documentation for these resources can be found here.</p>"},{"location":"user-guide/approve-dar/","title":"Approve DAR","text":""},{"location":"user-guide/approve-dar/#how-to-approve-data-access-request","title":"How to approve Data Access Request","text":"<p>Orchestration layer requires manual approvals before the GitHub workflow can start the data retrieval and data publishing phases. This solution uses GitHub Environments with protection rules. Currently, there is a single GitHub Team cr8-ALL-projects-Approver which is assigned to all DAR projects. This team is responsible for reviewing and approving Data Access Requests to ensure compliance with governance and access policies. Members of that group are notified (e.g. by email) when the workflow reaches Sign Off stage or Disclosure stage.</p> <p>Example email notification: </p> <p>The Approver should validate the Data Access Request files, mainly access, metadata and governance files and assess if the user can have the access to the underlying datasets. There is a Pull Request created which helps identifying the changes in the files, as well as artifact created which can be downloaded and explored locally.</p> <p></p> <p></p> <p>Once the approver is satisfied with the request, they should click on Review deployments, mark the signoff, and then click Approve and deploy. </p> <p>Alternatively, Approver can reject the stage, leaving a comment in the workflow or even providing comments directly in the Pull Request (suggesting the changes).</p>"},{"location":"user-guide/approve-dar/#how-to-add-new-members-to-approver-team","title":"How to add new members to Approver team","text":"<p>Navigate to GitHub Teams, click on cr8-ALL-projects-approver, then Add a member. If a member is not part of the Organisation, you need to invite him.</p> <p></p> <p>By default, team members have Read access to DAR project repositories.</p>"},{"location":"user-guide/create-new-dar-project/","title":"Creating a new DAR project","text":"<p>There are two ways we can create a new Data Access Request (DAR) project:</p> <p>1.) Cr8tor CLI <code>Initiate</code> command</p> <p>2.) GitHub Action Init RO-Crate project</p>"},{"location":"user-guide/create-new-dar-project/#cr8tor-cli-initiate-command","title":"Cr8tor CLI <code>Initiate</code> command","text":"<p>Details of <code>cr8tor initiate</code> command can be found here: Initiate command.</p> <p>Steps:</p> <ol> <li>Install uv and cr8tor cli following Readme</li> <li>Activate virtual environment.</li> <li>Change directory using <code>cd</code> command to the place you want to store your new project's folder.</li> <li>Run <code>cr8tor initiate</code> command providing template (<code>-t</code>) argument.</li> </ol> <p>Example - default, with cookiecutter prompts:</p> <pre><code>uv run  cr8tor  initiate -t \"https://github.com/lsc-sde-crates/cr8-cookiecutter\"\n</code></pre> <p>The command will download cookiecutter and start the prompt:    </p> <p>Example - without cookiecutter prompts</p> <pre><code>uv run  cr8tor  initiate -t \"https://github.com/lsc-sde-crates/cr8-cookiecutter\" -n \"project4\" -org \"lsc-sde-crates\" -e \"DEV\"\n</code></pre> <p>We can provide the project name as a parameter <code>-n</code>, as well as GitHub organisation <code>-org</code> and target environment <code>-e</code>.</p> <p>If we add <code>--push</code> argument, the application will try to create the remote repository and GitHub Teams and assign the correct GitHub's roles and permissions.</p> Warning <p>--push argument requires GitHub PAT token with the necessary organisation level permission. See minimum PAT token permissions defined here for token permission details. Store the token under GH_TOKEN Environment Variable (expected by gh_rest_api_client module)</p> <p>On successful run, we should see the new project's folder with sample access, governance and metadata files. If it is not already linked to the remote GitHub repository, link it. If remote does not exist, create/request it following below steps.    Follow update resources to progress with next steps.    </p> <p>Your local project should be linked to the remote repository. If it is not, follow the steps described in Local project folder not linked to remote GitHub repository</p> <p>By default, you cannot push directly to the main branch, but you need to create a pull request to it.</p>"},{"location":"user-guide/create-new-dar-project/#github-action-init-ro-crate-project","title":"GitHub Action <code>Init RO-Crate project</code>","text":"<p>If we do not have a PAT token or we do not want to use Cr8tor CLI Initate command to create for us GitHub's repository, then we can use the GitHub Action which runs the Initiate command for us. Workflow is located in the main cr8tor repository: Init RO-Crate project </p> <p>Then, we need to provide the requested project name</p> <p></p> <p>If the project name is e.g. <code>project001</code>, the GitHub repository created will be named <code>cr8-project001</code>.</p> <p></p> <p>Now, clone the repository to your local machine and update the resources toml files. Follow update resources</p> <p>By default, you cannot push directly to the main branch, but you need to create a pull request to it.</p> <p>If we notice devops_admin assigned to Reviewers, it means we changed the content of .github folder. It is secured by CODEOWNERS feature. See details in Changing files in .github folder </p>"},{"location":"user-guide/environments/","title":"Environments","text":""},{"location":"user-guide/environments/#overview","title":"Overview","text":"<p>Cr8tor cli app and Cr8tor-Publisher app are designed to extract the data from different source environments: DEV, TEST and PROD.</p> <p>When creating a new DAR project, we can choose the environment we want to work with. This will drive the GitHub runners and set of Organisation secrets that will be used during the main Orchestrator workflow run.</p> <p>As an example, choosing the DEV environment at the project level will pass the argument to the Orchestrator workflow. It will select the APPROVALS_API_TOKEN_DEV and APPROVAL_HOST_DEV secrets dedicated to that environment. Additionally, it will choose the runner dev-vm-01 (dev-cr8tor-vm), which can connect to the Databricks Unity Catalog within the dev-dedicated virtual network.</p> <p></p> <p>Example of cr8tor workflow environment use: </p> <p>Organisation secrets: </p> <p>Organisation runner: </p>"},{"location":"user-guide/orchestrate-dar/","title":"Orchestrate DAR","text":""},{"location":"user-guide/orchestrate-dar/#how-to-start-data-retrieval-process","title":"How to start data retrieval process","text":"<p>Once we have access, metadata and governance files prepared in the ./resources folder in our project's GitHub repository (main branch), we can start the Orchestrator workflow.</p> <p>In your GitHub repository, navigate to Actions -&gt; Orchestrate Data Load workflow. The workflow starts on demand, but the internal logic is specifically designed to work with the 'main' branch, ensuring proper execution and data consistency. Therefore, make sure to use the main branch and click Run workflow.</p> <p></p> <p></p> <p>The workflow executes 5 key stages:</p> <p>1) Validate</p> <p>2) SignOff</p> <p>3) Workflow-Execution</p> <p>4) Disclosure</p> <p>5) Publish</p>"},{"location":"user-guide/orchestrate-dar/#validate","title":"Validate","text":"<p>Validate stage runs two cr8tor cli commands:</p> <pre><code> cr8tor create\n cr8tor validate\n</code></pre> <p>Create command checks basic configuration and creates bagit and ro-crate metadata objects.</p> <p>Validate command initialises the connection to the source database (e.g. Databricks Unity Catalog), fetches the metadata of requested tables and columns and returns them back. Cr8tor makes here connections to the Approval Service endpoint project/validate.</p> <p>The results of commands are committed to a temporary git branch (like temp-20250305124305-validate) and a Pull Request from the temp branch to the main branch is created.</p>"},{"location":"user-guide/orchestrate-dar/#sign-off","title":"Sign Off","text":"<p>Sign Off stage uses GitHub repository environment with protection rules. It requires the approval from the assigned members. Currently, there is a single GitHub Team cr8-ALL-projects-approver which is assigned to all DAR projects. Members of that group are notified (e.g. by email) when the workflow reaches Sign Off stage.</p> <p>Example email notification: </p> <p>The approver should validate the Data Access Request files, focusing on access, metadata, and governance files. They should also assess whether the user can access the underlying datasets. There is a Pull Request created which helps identifying the changes in the files, as well as artifact created which can be downloaded and explored locally.</p> <p></p> <p></p> <p>Once approver is happy with the request, one should click on Review deployments, then mark signoff and click Approver and deploy. </p> <p>Alternatively, approver can reject the Sign Off stage, leaving a comment in the workflow or even providing comments directly in the Pull Request (suggesting the changes).</p> <p>Once Approved, the workflow runs</p> <pre><code> cr8tor sign-off\n</code></pre> <p>command which updates the ro-crate files with required logging and auditing logic (based on 5S-crate policy). Next, the Pull Request (from temp to main branch) is merged and a temp branch is deleted.</p>"},{"location":"user-guide/orchestrate-dar/#workflow-execution","title":"Workflow-Execution","text":"<p>Workflow-Execution stage runs</p> <pre><code> cr8tor stage-transfer\n</code></pre> <p>command which starts the data retrieval process. It makes a connection to the Approval Service project/package endpoint. Data is stored in the dedicated storage account in the staging container. Data is not yet available for the researcher, awaiting final approval in Disclosure process.</p> <p>Similarly to Validate stage, a new temp branch (like temp-20250305124305-stagetransfer) is created as well as a Pull Request to the main branch. </p>"},{"location":"user-guide/orchestrate-dar/#disclosure","title":"Disclosure","text":"<p>Likewise Sign Off stage, we request another review and approval from the GitHub Team cr8-ALL-projects-approver. Team is notified by email, and should verify the content of the files. Pull Request helps here identifying the changes between previous approval and now, as well as auditing trails logged in the ro-crate metadata files. One can also use artifact ro-crate-staged and analyse the files locally.</p> <p>When the approver is happy with the request, one should click on Review deployments, then mark signoff and click Approver and deploy.</p>"},{"location":"user-guide/orchestrate-dar/#publish","title":"Publish","text":"<p> Publish stage runs</p> <pre><code> cr8tor publish\n</code></pre> <p>command which starts the publishing process. It makes a connection to the Approval Service project/publish endpoint.</p> <p>Similarly to previous stages, a new temp branch (like temp-20250305124305-publish) is created as well as a Pull Request to the main branch. Pull Request is automatically merged to the main branch.</p> <p>If everything runs successfully, a final annotation should appear in the workflow Summary, and we can explore all artifacts created after each stage run.</p> <p></p> <p>We can also explore the files directly in the repository</p> <p></p>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":""},{"location":"user-guide/troubleshooting/#local-project-folder-not-linked-to-remote-github-repository","title":"Local project folder not linked to remote GitHub repository","text":"<p>If we created a new project using cr8tor cli initiate command, but not used --push argument, we might end up with the new project folder but unlinked to its remote GitHub repository. In this situation, first make sure that the remote GitHub repository exists. If it does not, request from Organisation's  administrator a new repository. Administrator needs the project name and environment you intend to use (PROD by default, DEV and TEST are dedicated for Developers). Administrator will run dedicated workflow as explain in Create a new DAR project - GitHub Action Init RO Crate Project.</p> <p>Then, we need to initiate the git. You can do it, e.g. using your IDE UI or with below git commands.</p> <pre><code>    cd &lt;my-project-folder&gt;\n    git init\n    git remote add origin &lt;remote-repository-url&gt;\n    git fetch\n    git checkout -b my-new-branch\n    git add .\n    git commit -m \"my initial changes\"\n    git merge origin/main --allow-unrelated-histories\n    git push --set-upstream origin my-new-branch\n</code></pre> <p>Resolve any merge conflicts between origin/main and your local branch, then push to remote branch and create a new Pull Request to main  branch.</p> <p>Alternative approach is to clone the remote repository first, and then replace relevant files.</p>"},{"location":"user-guide/troubleshooting/#changing-files-in-github-folder","title":"Changing files in .github folder","text":"<p>The changes to the files in the .github folder are restricted by the CODEOWNERS feature, which allows repository owners to define individuals or teams responsible for approving changes to specific files. Approval by the devops_admin GitHub team is required.</p> <p></p>"},{"location":"user-guide/troubleshooting/#databricks-api-errors-thrown-by-cr8tor","title":"Databricks API errors thrown by cr8tor","text":"<p>During Validation or WorkflowExecution(Stage-Transfer) jobs microservices and Cr8tor can return errors:</p> <ul> <li>Databricks API error: User does not have SELECT on &lt;table/schema&gt;,</li> <li>Databricks API error: User does not have USE SCHEMA on &lt;schema&gt;</li> </ul> <p>The error means that serviceprincipal specified in the access/access configuration does not have correct permission on the given datasource. Follow this guide for how to set up and assign correct permissions.</p>"},{"location":"user-guide/troubleshooting/#cr8tor-init-ro-crate-fails-with-401-client-error","title":"Cr8tor Init Ro-Crate fails with 401 client error","text":"<p>During Cr8tor Initiate command (init-project workflow) we use the GitHub Personal Access Token (PAT) to create new DAR project repository and create required GitHub Teams and set up repository settings.</p> <p>If the token expires, GitHub will return 401 Client Error: Unauthorized for url https://api.github.com/repos/my_organisation/cr8-my_repo</p> <p>See GitHub PAT token how to recreate the token and update relevant GitHub secrets.</p>"},{"location":"user-guide/update-resources-files/","title":"Updating DAR resource files","text":"<p>Every Data Access Request (DAR) project consists of below key files:</p> <p>1) config</p> <p>2) governance/project</p> <p>3) access/access</p> <p>4) metadata/dataset</p> <p></p> <p>They are all toml files. The <code>cr8tor</code> initiate command uses a cookie-cutter template which provides you with the basic version of them. The content needs to be updated with relevant information before you can successfully run the orchestration workflow.</p>"},{"location":"user-guide/update-resources-files/#config","title":"config","text":"<p>The <code>config</code> file contains the following fields, which must be populated with relevant information:</p> Field Description <code>Source-Organization</code> eg. <code>LSC SDE</code> <code>Organization-Address</code> eg. <code>Lancashire Teaching Hospitals NHS Trust, PR2 9HT</code> <code>Contact-Name</code> eg. <code>LSC SDE Program Team</code> <code>Contact-Email</code> eg. <code>lsc.sde@test.com</code>"},{"location":"user-guide/update-resources-files/#governanceproject","title":"governance/project","text":"<p>The <code>governance/project</code> contains the following fields, which must be populated with relevant information:</p> <p>[project]</p> Field Description <code>description</code> eg. <code>example-project created using CR8TOR.</code> <code>reference</code> auto-generated by cookie-cutter based on project_name. Pattern cr8-&lt;project_name&gt; <code>name</code> auto-generated by cookie-cutter based on project_name <code>project_name</code> follow organisation policy for project naming convention <p>[project.destination]</p> Field Description <code>type</code> Destination type. Supported values: [<code>filestore</code>, <code>postgresql</code>] <code>name</code> Destination name (for filestore destinations, this drives storage location), e.g., [<code>LSC</code>, <code>NW</code>] <code>format</code> Data format for filestore destinations. Supported values: [<code>csv</code>, <code>duckdb</code>]"},{"location":"user-guide/update-resources-files/#supported-destination-types","title":"Supported Destination Types","text":"<p>The CR8TOR Publisher supports two main destination types, each with different behaviors and requirements:</p>"},{"location":"user-guide/update-resources-files/#filestore-destination-filestore","title":"Filestore Destination (<code>filestore</code>)","text":"<p>When <code>type = \"filestore\"</code>, the Publish Service stores data as files in a mounted filestore rather than a database.</p> <p>Key Characteristics:</p> <ul> <li>File-based Storage: Data is stored as files (CSV or DuckDB format) in mounted filesystem paths</li> <li>Destination-Specific Storage: The <code>name</code> field drives storage location via environment variables (<code>TARGET_STORAGE_ACCOUNT_{NAME}_SDE_MNT_PATH</code>)</li> <li>Two-stage Process: Data flows through staging container first, then moves to production container</li> <li>BagIt Packaging: Files are organized following BagIt standards with SHA512 checksums for integrity verification</li> <li>Format Options: Supports both CSV and DuckDB formats for flexible data consumption</li> </ul> <p>Required Fields:</p> <ul> <li><code>type = \"filestore\"</code></li> <li><code>name</code> - Destination name (e.g., \"LSC\", \"NW\") that determines storage mount point</li> <li><code>format</code> - Data format: \"csv\" or \"duckdb\"</li> </ul> <p>Directory Structure Created:</p> <pre><code>{storage_mount_path}/\n\u251c\u2500\u2500 staging/\n\u2502   \u2514\u2500\u2500 {project_name}/\n\u2502       \u2514\u2500\u2500 {project_start_time}/\n\u2502           \u2514\u2500\u2500 data/outputs/\n\u2514\u2500\u2500 production/\n    \u2514\u2500\u2500 {project_name}/\n        \u2514\u2500\u2500 {project_start_time}/\n            \u2514\u2500\u2500 data/outputs/\n</code></pre>"},{"location":"user-guide/update-resources-files/#postgresql-destination-postgresql","title":"PostgreSQL Destination (<code>postgresql</code>)","text":"<p>When <code>type = \"postgresql\"</code>, the Publish Service loads data directly into a PostgreSQL database and creates OPAL resources for secure access.</p> <p>Key Characteristics:</p> <ul> <li>Database Storage: Data is loaded directly into PostgreSQL tables</li> <li>OPAL Integration: Automatically creates Obiba OPAL components for secure data access:</li> <li>Creates OPAL project named after the CR8TOR project</li> <li>Establishes OPAL resources pointing to PostgreSQL tables</li> <li>Creates DataSHIELD permission groups (<code>{project_name}_group</code>)</li> <li>Sets up DataSHIELD and resource-level permissions</li> <li>Access Control: Leverages OPAL's DataSHIELD framework for secure, privacy-preserving data analysis</li> <li>Table Filtering: Filters tables based on project name and start time patterns, excluding DLT internal tables</li> </ul> <p>Required Fields:</p> <ul> <li><code>type = \"postgresql\"</code></li> <li><code>name</code> - Destination name (used for OPAL project identification)</li> <li><code>format</code> - Not applicable for PostgreSQL destinations</li> </ul> <p>OPAL Resources Created:</p> <ul> <li>Resource Naming: <code>tre_postgresql_{schema}_{table}</code></li> <li>Project Structure: One OPAL project per CR8TOR project</li> <li>Group Management: DataSHIELD groups with \"use\" and \"view\" permissions</li> <li>User Management: Ensures default DataSHIELD user (<code>dsuser_default</code>) exists</li> </ul> <p>Choose <code>filestore</code> for file-based data delivery and <code>postgresql</code> for database-hosted data with integrated access control through OPAL's DataSHIELD framework.</p> <p>[repository]</p> Field Description <code>codeRepository</code> Auto-generated by cookie-cutter. e.g., <code>https://github.com/lsc-sde-crates/cr8-example-project</code> <code>description</code> Auto-generated by cookie-cutter. <code>name</code> Auto-generated by cookie-cutter. <p>[requesting_agent]</p> Field Description <code>name</code> e.g. Prof. Jane Doe <p>[requesting_agent.affiliation]</p> Field Description <code>url</code> e.g. https://someuni.com <code>name</code> e.g. Jane Doe's University"},{"location":"user-guide/update-resources-files/#accessaccess","title":"access/access","text":"<p>The <code>access/access</code> contains the following fields, which must be populated with relevant information:</p> <p>[source]</p> <p>The required source fields depend on the type of the source. The Solution supports extracting data from multiple source types including Databricks SQL warehouse endpoints and various SQL databases. The required fields for each type are validated by cr8tor and cr8tor-publisher app (using pydantic models).</p>"},{"location":"user-guide/update-resources-files/#supported-source-types","title":"Supported Source Types","text":"<p>The following source types are currently supported:</p> <ul> <li><code>databrickssql</code> - Databricks Unity Catalog</li> <li><code>postgresql</code> - PostgreSQL databases</li> <li><code>mysql</code> - MySQL databases</li> <li><code>mssql</code> - Microsoft SQL Server databases</li> <li><code>sqlserver</code> - Microsoft SQL Server databases (alternative identifier)</li> </ul>"},{"location":"user-guide/update-resources-files/#common-source-fields","title":"Common Source Fields","text":"Field Description <code>name</code> Name of the data source, e.g., <code>My Database Connection</code>. <code>type</code> Must match one of the supported source types listed above. <code>host_url</code> Host URL of the database server."},{"location":"user-guide/update-resources-files/#databricks-unity-catalog-databrickssql","title":"Databricks Unity Catalog (databrickssql)","text":"<p>For Databricks Unity Catalog sources, the following additional fields are required:</p> Field Description <code>http_path</code> HTTP path for the Databricks SQL Warehouse endpoint, e.g., <code>/sql/1.0/warehouses/0aec44b2e70e201d</code>. <code>port</code> Optional. Default is 443 for Databricks SQL endpoints. <code>catalog</code> Databricks Unity Catalog name from which data will be extracted."},{"location":"user-guide/update-resources-files/#sql-databases-postgresql-mysql-mssql","title":"SQL Databases (postgresql, mysql, mssql)","text":"<p>For SQL database sources, the following additional fields are required:</p> Field Description <code>database</code> Database name to connect to. <code>port</code> Database connection port (varies by database type). <p>[credentials]</p> <p>Credentials fields are tied to the source type. Different source types require different credential configurations:</p>"},{"location":"user-guide/update-resources-files/#databricks-unity-catalog-credentials","title":"Databricks Unity Catalog Credentials","text":"<p>For Databricks SQL endpoints, we use Databricks Service Principal (SPN). See here how to create a new SPN and assign required roles and permissions.</p> Field Description <code>provider</code> Credential provider, e.g., <code>AzureKeyVault</code>. <code>spn_clientid</code> Key name in secrets provider containing Service Principal client ID, e.g., <code>databricksspnclientid</code>. <code>spn_secret</code> Key name in secrets provider containing Service Principal secret, e.g., <code>databricksspnsecret</code>."},{"location":"user-guide/update-resources-files/#sql-database-credentials","title":"SQL Database Credentials","text":"<p>For SQL database sources (PostgreSQL, MySQL, MSSQL), we use username/password authentication:</p> Field Description <code>provider</code> Credential provider, e.g., <code>AzureKeyVault</code>. <code>username_key</code> Key name in secrets provider containing database username, e.g., <code>db-username</code>. <code>password_key</code> Key name in secrets provider containing database password, e.g., <code>db-password</code>."},{"location":"user-guide/update-resources-files/#metadatadataset","title":"metadata/dataset","text":"<p>We expect a single file per dataset (in terms of Databricks SQL source, it is per Unity Catalog schema). The <code>metadata/dataset</code> file is contains the following fields, which must be populated with relevant information:</p> Field Description <code>name</code> Name of the dataset, e.g., <code>dataset_1</code>. <code>description</code> Description of our dataset. <code>schema_name</code> Schema name, e.g., <code>z__cr8tor_poc</code>. Warning <p>Now, we can define the tables and columns we want to retrieve, but this is optional. If we do not provide tables and/or columns, the solution will retrieve all available tables and/or columns.</p> <p>[[tables]]</p> Field Description <code>name</code> Name of the table, e.g., <code>domain</code>. <p>[[tables.columns]]</p> Field Description <code>name</code> Name of the column, e.g., <code>domain_concept_id</code>."}]}